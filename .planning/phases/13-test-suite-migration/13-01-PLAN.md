---
phase: 13-test-suite-migration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/test_weighted_metrics.py
autonomous: true

must_haves:
  truths:
    - "All 11+ existing weighted metric tests pass (green CI)"
    - "Risk-weighted recall, precision, F2 calculations have test coverage"
    - "Float assertions use tolerance-based comparison (pytest.approx)"
    - "Tests validate frequency vs risk weight divergence behavior"
    - "Test suite runs in under 5 seconds"
  artifacts:
    - path: "tests/test_weighted_metrics.py"
      provides: "Complete test coverage for dual-weight recall framework"
      contains: "TestRiskWeightedMetrics"
      min_lines: 300
  key_links:
    - from: "tests/test_weighted_metrics.py"
      to: "tests/evaluate_presidio.py"
      via: "imports EvaluationMetrics"
      pattern: "from tests.evaluate_presidio import EvaluationMetrics"
    - from: "tests/test_weighted_metrics.py"
      to: "app/config.py"
      via: "imports settings for weight validation"
      pattern: "from app.config import settings"
---

<objective>
Migrate test suite from integer weights to float weights and add complete test coverage for the dual-weight recall framework.

Purpose: Validate that frequency-weighted and risk-weighted metrics calculate correctly with float precision, catching regressions that could silently reduce recall.

Output: All tests passing, complete coverage for risk-weighted methods, divergence validation tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-test-suite-migration/13-RESEARCH.md

# Source files being tested
@tests/test_weighted_metrics.py
@tests/evaluate_presidio.py
@app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix existing failing tests for float weights</name>
  <files>tests/test_weighted_metrics.py</files>
  <action>
Fix the 3 failing tests in TestWeightConfiguration class:

1. `test_weight_values_in_valid_range` (line 222-229):
   - Change `isinstance(weight, int)` to `isinstance(weight, (int, float))`
   - Change `assert weight <= 5` to `assert weight <= 5.0`
   - Update docstring: "non-negative integers" -> "non-negative floats"

2. `test_critical_entities_have_high_weights` (line 231-242):
   - Use pytest.approx() for all assertions
   - Update GUARDIAN_NAME from `== 5` to `== pytest.approx(4.0)` (current config value)
   - Update PERSON from `== 5` to `== pytest.approx(5.0)`
   - Update ROOM from `== 4` to `== pytest.approx(4.0)`
   - Update docstring to reflect "high weights" not specific values

3. `test_never_spoken_entities_have_zero_weight` (line 244-248):
   - Rename to `test_rarely_or_never_spoken_entities_have_low_weights`
   - Update EMAIL_ADDRESS from `== 0` to `== pytest.approx(0.0)`
   - Update LOCATION from `== 0` to `== pytest.approx(0.5)` (current config value)
   - Update PEDIATRIC_AGE to assert `== pytest.approx(0.0)` (the only true zero-weight entity in frequency weights)
   - Update docstring to reflect actual config: "EMAIL/PEDIATRIC_AGE never spoken (0.0), LOCATION rarely (0.5)"

Also update any other assertions in the file that use exact float comparison to use pytest.approx() - specifically in TestWeightedMetrics class methods that calculate expected values.
  </action>
  <verify>
```bash
python3 -m pytest tests/test_weighted_metrics.py::TestWeightConfiguration -v
# All 3 tests should pass
```
  </verify>
  <done>TestWeightConfiguration tests pass with float weight assertions</done>
</task>

<task type="auto">
  <name>Task 2: Add risk-weighted metric tests</name>
  <files>tests/test_weighted_metrics.py</files>
  <action>
Add a new TestRiskWeightedMetrics class after TestWeightedMetrics class with the following tests:

```python
class TestRiskWeightedMetrics:
    """Test risk-weighted metrics calculations (severity if leaked)."""

    def test_risk_weighted_recall_calculation(self):
        """Test risk-weighted recall with HIPAA severity weights."""
        entity_stats = {
            "MEDICAL_RECORD_NUMBER": {"tp": 70, "fn": 30, "fp": 10},  # 70% recall, risk=5.0
            "PERSON": {"tp": 95, "fn": 5, "fp": 10},                  # 95% recall, risk=5.0
        }

        risk_weights = {
            "MEDICAL_RECORD_NUMBER": 5.0,
            "PERSON": 5.0,
        }

        metrics = EvaluationMetrics()
        metrics.entity_stats = entity_stats

        risk_recall = metrics.risk_weighted_recall(risk_weights)

        # Manual calculation:
        # MRN: tp=70*5=350, total=100*5=500
        # PERSON: tp=95*5=475, total=100*5=500
        # risk_recall = (350+475)/(500+500) = 825/1000 = 0.825
        expected_recall = 825 / 1000

        assert risk_recall == pytest.approx(expected_recall)

    def test_risk_weighted_precision_calculation(self):
        """Test risk-weighted precision with HIPAA severity weights."""
        entity_stats = {
            "PHONE_NUMBER": {"tp": 80, "fn": 20, "fp": 20},  # 80% precision, risk=4.0
            "LOCATION": {"tp": 60, "fn": 40, "fp": 15},      # 80% precision, risk=4.0
        }

        risk_weights = {
            "PHONE_NUMBER": 4.0,
            "LOCATION": 4.0,
        }

        metrics = EvaluationMetrics()
        metrics.entity_stats = entity_stats

        risk_precision = metrics.risk_weighted_precision(risk_weights)

        # Manual calculation:
        # PHONE: tp=80*4=320, detected=100*4=400
        # LOCATION: tp=60*4=240, detected=75*4=300
        # precision = (320+240)/(400+300) = 560/700 = 0.8
        expected_precision = 560 / 700

        assert risk_precision == pytest.approx(expected_precision)

    def test_risk_weighted_f2_calculation(self):
        """Test risk-weighted F2 score (recall-weighted)."""
        entity_stats = {
            "GUARDIAN_NAME": {"tp": 85, "fn": 15, "fp": 10},  # risk=4.0
        }

        risk_weights = {"GUARDIAN_NAME": 4.0}

        metrics = EvaluationMetrics()
        metrics.entity_stats = entity_stats

        risk_f2 = metrics.risk_weighted_f2(risk_weights)

        # Calculate expected F2
        # precision = 85/95 = 0.894...
        # recall = 85/100 = 0.85
        # F2 = (1 + 4) * (p * r) / (4 * p + r)
        p = 85 / 95
        r = 85 / 100
        beta = 2.0
        expected_f2 = (1 + beta**2) * (p * r) / (beta**2 * p + r)

        assert risk_f2 == pytest.approx(expected_f2)

    def test_risk_weights_loaded_from_config(self):
        """Test that risk weights are properly loaded from config."""
        risk_weights = settings.spoken_handoff_risk_weights

        # Verify all expected entities have risk weights
        expected_entities = [
            "PERSON",
            "GUARDIAN_NAME",
            "ROOM",
            "PHONE_NUMBER",
            "DATE_TIME",
            "MEDICAL_RECORD_NUMBER",
            "EMAIL_ADDRESS",
            "LOCATION",
            "PEDIATRIC_AGE",
        ]

        for entity in expected_entities:
            assert entity in risk_weights, f"{entity} missing from risk weights config"

    def test_risk_weight_values_in_valid_range(self):
        """Test that all risk weights are non-negative floats in valid range."""
        risk_weights = settings.spoken_handoff_risk_weights

        for entity, weight in risk_weights.items():
            assert isinstance(weight, (int, float)), f"{entity} risk weight is not numeric"
            assert weight >= 0.0, f"{entity} risk weight is negative"
            assert weight <= 5.0, f"{entity} risk weight exceeds maximum of 5"
```

Import `settings` from app.config at the top of the file if not already imported.
  </action>
  <verify>
```bash
python3 -m pytest tests/test_weighted_metrics.py::TestRiskWeightedMetrics -v
# All 5 tests should pass
```
  </verify>
  <done>Risk-weighted recall, precision, and F2 have test coverage with config validation</done>
</task>

<task type="auto">
  <name>Task 3: Add frequency vs risk divergence validation tests</name>
  <files>tests/test_weighted_metrics.py</files>
  <action>
Add a new TestWeightDivergence class after TestRiskWeightedMetrics:

```python
class TestWeightDivergence:
    """Test that frequency and risk weights can diverge appropriately."""

    def test_mixed_entities_frequency_vs_risk_divergence(self):
        """Test that frequency and risk can produce different metric values.

        Key insight: MRN has low frequency weight (0.5) but high risk weight (5.0).
        When MRN performs poorly, frequency-weighted recall stays high (dominated by PERSON),
        but risk-weighted recall drops (MRN has equal weight to PERSON).
        """
        entity_stats = {
            "MEDICAL_RECORD_NUMBER": {"tp": 30, "fn": 70, "fp": 5},   # 30% recall
            "PERSON": {"tp": 95, "fn": 5, "fp": 10},                  # 95% recall
        }

        freq_weights = {
            "MEDICAL_RECORD_NUMBER": 0.5,   # Rarely spoken
            "PERSON": 5.0,                  # Always spoken
        }

        risk_weights = {
            "MEDICAL_RECORD_NUMBER": 5.0,   # Critical if leaked
            "PERSON": 5.0,                  # Critical if leaked
        }

        metrics = EvaluationMetrics()
        metrics.entity_stats = entity_stats

        freq_recall = metrics.weighted_recall(freq_weights)
        risk_recall = metrics.risk_weighted_recall(risk_weights)

        # Frequency-weighted: dominated by high-performing PERSON (weight 5.0)
        # MRN: tp=30*0.5=15, total=100*0.5=50
        # PERSON: tp=95*5=475, total=100*5=500
        # freq = (15+475)/(50+500) = 490/550 = 0.8909...
        assert freq_recall == pytest.approx(490 / 550)

        # Risk-weighted: MRN has equal weight (5.0), drags down average
        # MRN: tp=30*5=150, total=100*5=500
        # PERSON: tp=95*5=475, total=100*5=500
        # risk = (150+475)/(500+500) = 625/1000 = 0.625
        assert risk_recall == pytest.approx(625 / 1000)

        # Verify divergence: frequency > risk when high-risk entity underperforms
        assert freq_recall > risk_recall
        assert abs(freq_recall - risk_recall) > 0.2  # Significant difference

    def test_zero_weight_entities_invisible_in_weighted_visible_in_unweighted(self):
        """Test that zero-weight entities are invisible in weighted but visible in unweighted.

        This is the HIPAA safety check: unweighted recall must always be reported
        as the safety floor because it catches zero-weight entity failures.
        """
        entity_stats = {
            "PERSON": {"tp": 100, "fn": 0, "fp": 10},       # 100% recall, weight 5.0
            "PEDIATRIC_AGE": {"tp": 0, "fn": 50, "fp": 0},  # 0% recall, weight 0.0
        }

        freq_weights = {
            "PERSON": 5.0,
            "PEDIATRIC_AGE": 0.0,
        }

        metrics = EvaluationMetrics()
        metrics.entity_stats = entity_stats

        # Unweighted recall includes PEDIATRIC_AGE failures
        # tp=100, fn=50, recall = 100/150 = 66.7%
        total_tp = 100
        total_fn = 0 + 50
        unweighted_recall = total_tp / (total_tp + total_fn)
        assert unweighted_recall == pytest.approx(100 / 150)

        # Weighted recall ignores zero-weight PEDIATRIC_AGE
        # Only PERSON counts: 100% recall
        weighted_recall = metrics.weighted_recall(freq_weights)
        assert weighted_recall == pytest.approx(1.0)

        # Safety check: unweighted is visible and lower
        # This is why we must always report unweighted as safety floor
        assert unweighted_recall < weighted_recall

    def test_actual_config_weights_show_expected_divergence_pattern(self):
        """Test that actual config weights produce expected divergence with realistic data.

        Uses real config values to ensure the weight scheme works as designed.
        Expected pattern: frequency-weighted > risk-weighted when MRN underperforms
        (because MRN has freq=0.5 but risk=5.0).
        """
        # Simulate realistic scenario: names detected well, MRN detected poorly
        entity_stats = {
            "PERSON": {"tp": 95, "fn": 5, "fp": 10},              # 95% recall
            "GUARDIAN_NAME": {"tp": 85, "fn": 15, "fp": 5},       # 85% recall
            "MEDICAL_RECORD_NUMBER": {"tp": 40, "fn": 60, "fp": 2}, # 40% recall (poor)
            "ROOM": {"tp": 80, "fn": 20, "fp": 10},               # 80% recall
        }

        freq_weights = settings.spoken_handoff_weights
        risk_weights = settings.spoken_handoff_risk_weights

        metrics = EvaluationMetrics()
        metrics.entity_stats = entity_stats

        freq_recall = metrics.weighted_recall(freq_weights)
        risk_recall = metrics.risk_weighted_recall(risk_weights)

        # Frequency-weighted should be higher (MRN has low weight 0.5)
        # Risk-weighted should be lower (MRN has high weight 5.0, drags down average)
        assert freq_recall > risk_recall, \
            f"Expected freq ({freq_recall:.3f}) > risk ({risk_recall:.3f}) when MRN underperforms"

    def test_all_zero_weights_return_zero(self):
        """Test that all-zero weights return 0.0 (not division by zero)."""
        entity_stats = {
            "EMAIL_ADDRESS": {"tp": 50, "fn": 0, "fp": 0},
            "PEDIATRIC_AGE": {"tp": 60, "fn": 0, "fp": 0},
        }

        weights = {
            "EMAIL_ADDRESS": 0.0,
            "PEDIATRIC_AGE": 0.0,
        }

        metrics = EvaluationMetrics()
        metrics.entity_stats = entity_stats

        # Should return 0.0, not raise ZeroDivisionError
        assert metrics.weighted_recall(weights) == 0.0
        assert metrics.weighted_precision(weights) == 0.0
        assert metrics.weighted_f2(weights) == 0.0
```

Note: The test `test_zero_weight_entities_invisible_in_weighted_visible_in_unweighted` checks the manually calculated unweighted recall against the expected value, not the metrics.recall property (which requires metrics._y_true/_y_pred to be set). This keeps the test focused on the weighted calculation behavior.
  </action>
  <verify>
```bash
python3 -m pytest tests/test_weighted_metrics.py::TestWeightDivergence -v
# All 4 tests should pass
```
  </verify>
  <done>Divergence validation tests confirm frequency and risk weights behave correctly</done>
</task>

</tasks>

<verification>
After all tasks complete:

```bash
# Run full test suite
python3 -m pytest tests/test_weighted_metrics.py -v

# Verify performance (must be under 5 seconds)
python3 -m pytest tests/test_weighted_metrics.py --durations=0

# Run as part of full test suite to ensure no regressions
python3 -m pytest tests/ -v --ignore=tests/real_handoffs/ -x
```

Expected outcomes:
- All tests pass (0 failures)
- Total test count: ~20 tests (11 existing + 5 risk-weighted + 4 divergence)
- Execution time: < 5 seconds (target: < 1 second for unit tests)
</verification>

<success_criteria>
Phase 13 requirements satisfied:
- [x] TEST-01: Existing weighted metric tests pass with float weights
- [x] TEST-02: Risk-weighted recall calculation has test coverage
- [x] TEST-03: Risk-weighted precision calculation has test coverage
- [x] TEST-04: Risk-weighted F2 calculation has test coverage
- [x] TEST-05: Float assertions use tolerance (pytest.approx)
- [x] TEST-06: Test validates frequency vs risk weight divergence behavior

Additional success criteria from ROADMAP:
- All existing weighted metric tests pass with float weights (not int)
- Risk-weighted recall/precision/F2 calculations have test coverage
- Float assertions use tolerance-based comparison (not exact equality)
- Tests validate frequency vs risk weight divergence behavior
- Test suite runs in under 5 seconds (unit tests only, no audio)
</success_criteria>

<output>
After completion, create `.planning/phases/13-test-suite-migration/13-01-SUMMARY.md`
</output>
