---
phase: 05-validation-compliance
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/evaluate_presidio.py
  - tests/error_taxonomy.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Clinician can see confidence intervals around recall/precision numbers in validation output"
    - "Clinician can see why specific PHI was missed, categorized by failure type"
    - "Validation report shows uncertainty bounds, not just point estimates"
  artifacts:
    - path: "tests/evaluate_presidio.py"
      provides: "bootstrap_recall_ci, bootstrap_precision_ci methods"
      exports: ["EvaluationMetrics.recall_ci", "EvaluationMetrics.precision_ci"]
    - path: "tests/error_taxonomy.py"
      provides: "False negative classification by failure mode"
      exports: ["FailureMode", "classify_failure", "build_error_taxonomy"]
  key_links:
    - from: "tests/run_validation.py"
      to: "tests/error_taxonomy.py"
      via: "Error taxonomy import for false negative analysis"
      pattern: "from error_taxonomy import"
---

<objective>
Add bootstrap confidence intervals and error taxonomy to evaluation infrastructure

Purpose: Clinical deployment requires statistical validation (95% CI for recall) and actionable error analysis (categorize why PHI was missed). These metrics are required for HIPAA Expert Determination compliance documentation.

Output: Enhanced EvaluationMetrics with confidence intervals, error taxonomy module for false negative classification, and CLI support for generating confidence intervals.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-validation-compliance/05-RESEARCH.md
@tests/evaluate_presidio.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create error taxonomy module for false negative classification</name>
  <files>tests/error_taxonomy.py</files>
  <action>
Create tests/error_taxonomy.py with:

1. FailureMode enum:
```python
class FailureMode(Enum):
    PATTERN_MISS = "pattern_miss"        # Regex didn't match variant
    THRESHOLD_MISS = "threshold_miss"    # Detected but score below threshold
    DENY_LIST_FILTER = "deny_list_filtered"  # Incorrectly filtered by deny list
    NOVEL_VARIANT = "novel_variant"      # Pattern not in synthetic training
    SPAN_BOUNDARY = "span_boundary"      # Partial match only
    NER_MISS = "ner_miss"               # spaCy NER didn't recognize entity
```

2. FalseNegativeCase dataclass:
```python
@dataclass
class FalseNegativeCase:
    entity_type: str
    text: str
    context: str  # 50 chars before/after
    start: int
    end: int
    failure_mode: FailureMode
    detected_partial: Optional[dict] = None  # If partial match exists
```

3. classify_failure(fn_span, detected_spans, text, threshold=0.30, deny_lists=None) function:
- Check for partial overlap (SPAN_BOUNDARY)
- Check against deny list terms (DENY_LIST_FILTER)
- Check if novel pattern not in synthetic templates (NOVEL_VARIANT)
- Default to PATTERN_MISS

4. build_error_taxonomy(results: list[DetectionResult]) -> dict[FailureMode, list[FalseNegativeCase]]:
- Iterates through all false negatives
- Classifies each and groups by failure mode
- Returns taxonomy dict

5. generate_error_report(taxonomy: dict) -> str:
- Formats taxonomy as readable report
- Shows counts per failure mode
- Lists top 5 examples per category
- Includes actionable recommendations per failure mode
  </action>
  <verify>
```bash
python -c "from tests.error_taxonomy import FailureMode, classify_failure, build_error_taxonomy; print(f'Failure modes: {[m.value for m in FailureMode]}')"
```
  </verify>
  <done>
- FailureMode enum with 6 failure modes defined
- classify_failure returns appropriate FailureMode for a false negative
- build_error_taxonomy groups false negatives by failure mode
- generate_error_report produces actionable analysis
  </done>
</task>

<task type="auto">
  <name>Task 2: Add bootstrap confidence intervals to EvaluationMetrics</name>
  <files>tests/evaluate_presidio.py</files>
  <action>
Enhance tests/evaluate_presidio.py:

1. Add imports at top:
```python
import numpy as np
from typing import Tuple
```

2. Add to EvaluationMetrics class:

```python
def bootstrap_recall_ci(
    self,
    y_true: np.ndarray,
    y_pred: np.ndarray,
    n_bootstrap: int = 10000,
    confidence: float = 0.95,
    seed: int = 42
) -> Tuple[float, Tuple[float, float]]:
    """
    Calculate bootstrap CI for recall using percentile method.

    Args:
        y_true: Binary array (1 = PHI present, 0 = no PHI)
        y_pred: Binary array (1 = detected, 0 = not detected)
        n_bootstrap: Number of bootstrap iterations
        confidence: Confidence level (default 0.95)
        seed: Random seed for reproducibility

    Returns:
        Tuple of (point_estimate, (ci_lower, ci_upper))
    """
    np.random.seed(seed)
    n = len(y_true)
    recalls = []

    for _ in range(n_bootstrap):
        indices = np.random.choice(n, size=n, replace=True)
        y_true_boot = y_true[indices]
        y_pred_boot = y_pred[indices]

        tp = np.sum((y_true_boot == 1) & (y_pred_boot == 1))
        fn = np.sum((y_true_boot == 1) & (y_pred_boot == 0))
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        recalls.append(recall)

    alpha = 1 - confidence
    lower = np.percentile(recalls, 100 * alpha / 2)
    upper = np.percentile(recalls, 100 * (1 - alpha / 2))

    return np.mean(recalls), (lower, upper)

def bootstrap_precision_ci(
    self,
    y_true: np.ndarray,
    y_pred: np.ndarray,
    n_bootstrap: int = 10000,
    confidence: float = 0.95,
    seed: int = 42
) -> Tuple[float, Tuple[float, float]]:
    """Calculate bootstrap CI for precision."""
    np.random.seed(seed)
    n = len(y_true)
    precisions = []

    for _ in range(n_bootstrap):
        indices = np.random.choice(n, size=n, replace=True)
        y_true_boot = y_true[indices]
        y_pred_boot = y_pred[indices]

        tp = np.sum((y_true_boot == 1) & (y_pred_boot == 1))
        fp = np.sum((y_true_boot == 0) & (y_pred_boot == 1))
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        precisions.append(precision)

    alpha = 1 - confidence
    lower = np.percentile(precisions, 100 * alpha / 2)
    upper = np.percentile(precisions, 100 * (1 - alpha / 2))

    return np.mean(precisions), (lower, upper)
```

3. Add to PresidioEvaluator.evaluate_dataset():
After computing metrics, add:
```python
# Build binary arrays for CI calculation
y_true = []
y_pred = []
for result in results:
    for _ in result.true_positives:
        y_true.append(1)
        y_pred.append(1)
    for _ in result.false_negatives:
        y_true.append(1)
        y_pred.append(0)
    for _ in result.false_positives:
        y_true.append(0)
        y_pred.append(1)

metrics._y_true = np.array(y_true)
metrics._y_pred = np.array(y_pred)
```

4. Add CLI flag --with-ci to main():
```python
parser.add_argument(
    "--with-ci",
    action="store_true",
    help="Calculate bootstrap 95%% confidence intervals (slower)"
)
```

5. In generate_report(), if CI data available, add:
```python
if hasattr(metrics, '_y_true') and len(metrics._y_true) > 0:
    recall_mean, (recall_lower, recall_upper) = metrics.bootstrap_recall_ci(
        metrics._y_true, metrics._y_pred
    )
    prec_mean, (prec_lower, prec_upper) = metrics.bootstrap_precision_ci(
        metrics._y_true, metrics._y_pred
    )
    lines.append(f"  Recall 95% CI:    [{recall_lower:.1%}, {recall_upper:.1%}]")
    lines.append(f"  Precision 95% CI: [{prec_lower:.1%}, {prec_upper:.1%}]")
```
  </action>
  <verify>
```bash
cd "/Users/joshpankin/My Drive/10-19 Projects/12 Development & AI Projects/12.09 Pediatric_Handoff_PHI_Remover"
python -c "
from tests.evaluate_presidio import EvaluationMetrics
import numpy as np

# Test bootstrap CI calculation
m = EvaluationMetrics(total_expected=100, total_detected=95, true_positives=90, false_negatives=10, false_positives=5)
y_true = np.array([1]*100)  # 100 expected
y_pred = np.array([1]*90 + [0]*10)  # 90 detected, 10 missed
recall, (lower, upper) = m.bootstrap_recall_ci(y_true, y_pred, n_bootstrap=1000)
print(f'Recall: {recall:.1%} (95% CI: {lower:.1%} - {upper:.1%})')
"
```
  </verify>
  <done>
- bootstrap_recall_ci method returns (point_estimate, (ci_lower, ci_upper))
- bootstrap_precision_ci method returns same format
- CLI --with-ci flag calculates and displays confidence intervals
- Report includes CI when available
  </done>
</task>

</tasks>

<verification>
1. Error taxonomy module imports and classifies failures
2. Bootstrap CI calculation produces reasonable intervals
3. CLI with --with-ci flag works

```bash
cd "/Users/joshpankin/My Drive/10-19 Projects/12 Development & AI Projects/12.09 Pediatric_Handoff_PHI_Remover"

# Test error taxonomy
python -c "
from tests.error_taxonomy import FailureMode, FalseNegativeCase
case = FalseNegativeCase(
    entity_type='PERSON',
    text='John Smith',
    context='Patient John Smith was',
    start=8,
    end=18,
    failure_mode=FailureMode.PATTERN_MISS
)
print(f'Case: {case.text} - {case.failure_mode.value}')
"

# Test CI calculation
python tests/evaluate_presidio.py --input tests/synthetic_handoffs.json --with-ci 2>&1 | head -30
```
</verification>

<success_criteria>
- [ ] tests/error_taxonomy.py imports without error
- [ ] FailureMode enum has 6 failure modes
- [ ] classify_failure returns FailureMode for given false negative
- [ ] bootstrap_recall_ci returns tuple with point estimate and CI bounds
- [ ] bootstrap_precision_ci returns tuple with point estimate and CI bounds
- [ ] CLI --with-ci flag produces output with confidence intervals
</success_criteria>

<output>
After completion, create `.planning/phases/05-validation-compliance/05-02-SUMMARY.md`
</output>
