---
phase: 05-validation-compliance
plan: 03
type: execute
wave: 2
depends_on: ["05-01", "05-02"]
files_modified:
  - tests/run_validation.py
  - .planning/phases/05-validation-compliance/VALIDATION_REPORT.md
autonomous: false
user_setup: []

must_haves:
  truths:
    - "Validation script runs against synthetic data (proxy for real until IRB data available)"
    - "HIPAA compliance metrics calculated with 95% CI"
    - "Validation report documents methodology and results"
  artifacts:
    - path: "tests/run_validation.py"
      provides: "Main validation runner script"
      exports: ["run_validation", "generate_compliance_report"]
    - path: ".planning/phases/05-validation-compliance/VALIDATION_REPORT.md"
      provides: "Complete validation results and compliance documentation"
      contains: "Recall"
  key_links:
    - from: "tests/run_validation.py"
      to: "tests/evaluate_presidio.py"
      via: "PresidioEvaluator and bootstrap CI"
      pattern: "from evaluate_presidio import"
    - from: "tests/run_validation.py"
      to: "tests/error_taxonomy.py"
      via: "Error analysis"
      pattern: "from error_taxonomy import"
---

<objective>
Run validation evaluation and generate compliance documentation

Purpose: Execute validation on available data (synthetic corpus as proxy until real transcripts available), calculate recall with 95% CI, generate HIPAA-compliant documentation, and determine if 95% recall threshold is met.

Output: Validation runner script, complete VALIDATION_REPORT.md with metrics, CI, error taxonomy, and deployment readiness assessment.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-validation-compliance/05-RESEARCH.md
@.planning/phases/05-validation-compliance/05-CONTEXT.md
@.planning/phases/05-validation-compliance/05-01-SUMMARY.md
@.planning/phases/05-validation-compliance/05-02-SUMMARY.md
@tests/evaluate_presidio.py
@tests/error_taxonomy.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create validation runner script</name>
  <files>tests/run_validation.py</files>
  <action>
Create tests/run_validation.py as main validation orchestrator:

```python
#!/usr/bin/env python
"""
Run validation evaluation and generate compliance documentation.

Usage:
    python tests/run_validation.py --output validation_results.json
    python tests/run_validation.py --report .planning/phases/05-validation-compliance/VALIDATION_REPORT.md
"""

import argparse
import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from tests.evaluate_presidio import PresidioEvaluator, EvaluationMetrics
from tests.generate_test_data import load_dataset
from tests.error_taxonomy import build_error_taxonomy, generate_error_report, FailureMode

# Try to import validation dataset (may not exist in Phase 5 Plan 1)
try:
    from tests.validation_dataset import load_validation_set
    HAS_VALIDATION_DATASET = True
except ImportError:
    HAS_VALIDATION_DATASET = False


def run_validation(
    input_path: Path,
    use_real_data: bool = False,
    overlap_threshold: float = 0.5,
    n_bootstrap: int = 10000,
    seed: int = 42
) -> dict:
    """
    Run full validation evaluation.

    Args:
        input_path: Path to dataset (synthetic or real)
        use_real_data: If True, use validation_dataset loader
        overlap_threshold: Span matching threshold
        n_bootstrap: Bootstrap iterations for CI
        seed: Random seed

    Returns:
        Dict with metrics, CI, error taxonomy
    """
    # Load dataset
    if use_real_data and HAS_VALIDATION_DATASET:
        dataset = load_validation_set(input_path)
    else:
        dataset = load_dataset(input_path)

    # Run evaluation
    evaluator = PresidioEvaluator(overlap_threshold=overlap_threshold)
    metrics, results = evaluator.evaluate_dataset(dataset, verbose=True)

    # Calculate confidence intervals
    import numpy as np
    y_true = []
    y_pred = []
    for result in results:
        for _ in result.true_positives:
            y_true.append(1)
            y_pred.append(1)
        for _ in result.false_negatives:
            y_true.append(1)
            y_pred.append(0)
        for _ in result.false_positives:
            y_true.append(0)
            y_pred.append(1)

    y_true = np.array(y_true)
    y_pred = np.array(y_pred)

    recall_mean, (recall_lower, recall_upper) = metrics.bootstrap_recall_ci(
        y_true, y_pred, n_bootstrap=n_bootstrap, seed=seed
    )
    prec_mean, (prec_lower, prec_upper) = metrics.bootstrap_precision_ci(
        y_true, y_pred, n_bootstrap=n_bootstrap, seed=seed
    )

    # Build error taxonomy
    error_taxonomy = build_error_taxonomy(results)
    error_counts = {mode.value: len(cases) for mode, cases in error_taxonomy.items()}

    # Check deployment threshold
    DEPLOYMENT_THRESHOLD = 0.95
    meets_threshold = recall_lower >= DEPLOYMENT_THRESHOLD

    return {
        "timestamp": datetime.now().isoformat(),
        "dataset": {
            "path": str(input_path),
            "type": "real" if use_real_data else "synthetic",
            "n_handoffs": len(dataset),
            "n_phi_spans": metrics.total_expected
        },
        "metrics": {
            "recall": metrics.recall,
            "recall_ci_lower": recall_lower,
            "recall_ci_upper": recall_upper,
            "precision": metrics.precision,
            "precision_ci_lower": prec_lower,
            "precision_ci_upper": prec_upper,
            "f1": metrics.f1,
            "f2": metrics.f2,
            "true_positives": metrics.true_positives,
            "false_negatives": metrics.false_negatives,
            "false_positives": metrics.false_positives
        },
        "error_taxonomy": error_counts,
        "deployment_readiness": {
            "threshold": DEPLOYMENT_THRESHOLD,
            "meets_threshold": meets_threshold,
            "decision": "DEPLOY" if meets_threshold else "RETURN_TO_PHASE_4"
        },
        "methodology": {
            "overlap_threshold": overlap_threshold,
            "n_bootstrap": n_bootstrap,
            "seed": seed,
            "confidence_level": 0.95
        }
    }


def generate_compliance_report(results: dict, output_path: Path) -> str:
    """Generate HIPAA compliance documentation."""
    m = results["metrics"]
    d = results["dataset"]
    dep = results["deployment_readiness"]

    report = f"""# Validation & Compliance Report

**Generated:** {results["timestamp"]}
**Dataset:** {d["type"]} ({d["n_handoffs"]} handoffs, {d["n_phi_spans"]} PHI spans)

## Executive Summary

| Metric | Value | 95% CI |
|--------|-------|--------|
| **Recall** | {m["recall"]:.1%} | [{m["recall_ci_lower"]:.1%}, {m["recall_ci_upper"]:.1%}] |
| Precision | {m["precision"]:.1%} | [{m["precision_ci_lower"]:.1%}, {m["precision_ci_upper"]:.1%}] |
| F1 Score | {m["f1"]:.1%} | - |
| F2 Score | {m["f2"]:.1%} | - |

**Deployment Threshold:** {dep["threshold"]:.0%} recall (lower bound of 95% CI)
**Decision:** {dep["decision"]}

{"PASS: System meets 95% recall threshold. Ready for MVP deployment." if dep["meets_threshold"] else "FAIL: Recall CI lower bound below 95%. Return to Phase 4 for pattern improvements."}

## Detailed Metrics

### Detection Performance

| Category | Count |
|----------|-------|
| Total expected PHI spans | {m["true_positives"] + m["false_negatives"]} |
| True positives | {m["true_positives"]} |
| False negatives (PHI leaks) | {m["false_negatives"]} |
| False positives (over-redaction) | {m["false_positives"]} |

### Error Taxonomy

| Failure Mode | Count | Description |
|--------------|-------|-------------|
"""

    taxonomy_descriptions = {
        "pattern_miss": "Regex pattern didn't match variant",
        "threshold_miss": "Detected but score below threshold",
        "deny_list_filtered": "Incorrectly filtered by deny list",
        "novel_variant": "Pattern not in synthetic training data",
        "span_boundary": "Partial match only (boundary issue)",
        "ner_miss": "spaCy NER didn't recognize entity"
    }

    for mode, count in sorted(results["error_taxonomy"].items(), key=lambda x: x[1], reverse=True):
        desc = taxonomy_descriptions.get(mode, "Unknown")
        report += f"| {mode} | {count} | {desc} |\n"

    report += f"""
## Methodology

### Evaluation Protocol

1. **Dataset**: {d["type"]} corpus with {d["n_handoffs"]} handoffs
2. **Entity types evaluated**: PERSON, PHONE_NUMBER, EMAIL_ADDRESS, DATE_TIME, LOCATION, MEDICAL_RECORD_NUMBER, ROOM, PEDIATRIC_AGE, GUARDIAN_NAME
3. **Span matching**: Overlap threshold = {results["methodology"]["overlap_threshold"]}
4. **Confidence intervals**: Bootstrap percentile method ({results["methodology"]["n_bootstrap"]:,} iterations, seed={results["methodology"]["seed"]})

### HIPAA Compliance

This evaluation follows the **Expert Determination** standard under HIPAA Privacy Rule (45 CFR 164.514(b)):

1. **Statistical method**: Bootstrap percentile confidence intervals
2. **Threshold**: >95% recall with 95% confidence
3. **Residual risk**: Estimated at {(1 - m["recall_ci_lower"]) * 100:.2f}% of PHI may remain in output

### Limitations

1. **Synthetic data proxy**: Current validation uses synthetic handoffs; external validation on real transcripts pending IRB approval
2. **Domain shift**: Performance on real clinical transcripts may differ from synthetic (literature suggests 15-30% gap)
3. **Single annotator**: Ground truth from synthetic generation, not human expert annotation

## Recommendations

"""

    if dep["meets_threshold"]:
        report += """1. **Proceed to MVP deployment** for personal use
2. Collect real transcript data for external validation (IRB-dependent)
3. Monitor false negative patterns during production use
4. Re-validate after collecting 50+ real handoffs
"""
    else:
        report += f"""1. **Return to Phase 4** for pattern improvements
2. Focus on failure modes with highest counts:
"""
        for mode, count in sorted(results["error_taxonomy"].items(), key=lambda x: x[1], reverse=True)[:3]:
            report += f"   - {mode}: {count} cases\n"
        report += """3. Re-run validation after improvements
4. Target: Achieve recall CI lower bound >= 95%
"""

    report += f"""
---

*Validation completed: {results["timestamp"]}*
*Methodology: Bootstrap CI ({results["methodology"]["n_bootstrap"]:,} iterations) with overlap threshold {results["methodology"]["overlap_threshold"]}*
"""

    # Write report
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(report)

    return report


def main():
    parser = argparse.ArgumentParser(description="Run validation evaluation")
    parser.add_argument(
        "--input", "-i",
        type=Path,
        default=Path("tests/synthetic_handoffs.json"),
        help="Input dataset (default: synthetic_handoffs.json)"
    )
    parser.add_argument(
        "--real-data",
        action="store_true",
        help="Use real data loader (validation_dataset.py)"
    )
    parser.add_argument(
        "--output", "-o",
        type=Path,
        help="Output JSON results file"
    )
    parser.add_argument(
        "--report", "-r",
        type=Path,
        help="Output markdown report path"
    )
    parser.add_argument(
        "--n-bootstrap",
        type=int,
        default=10000,
        help="Number of bootstrap iterations (default: 10000)"
    )

    args = parser.parse_args()

    if not args.input.exists():
        print(f"Error: Input file not found: {args.input}")
        sys.exit(1)

    print(f"Running validation on {args.input}...")
    results = run_validation(
        args.input,
        use_real_data=args.real_data,
        n_bootstrap=args.n_bootstrap
    )

    # Output JSON
    if args.output:
        args.output.parent.mkdir(parents=True, exist_ok=True)
        with open(args.output, "w") as f:
            json.dump(results, f, indent=2)
        print(f"Results saved to {args.output}")

    # Output report
    if args.report:
        report = generate_compliance_report(results, args.report)
        print(f"Report saved to {args.report}")
        print("\n" + "=" * 60)
        print(report[:2000] + "..." if len(report) > 2000 else report)

    # Print summary
    m = results["metrics"]
    dep = results["deployment_readiness"]
    print("\n" + "=" * 60)
    print("VALIDATION SUMMARY")
    print("=" * 60)
    print(f"Recall: {m['recall']:.1%} (95% CI: {m['recall_ci_lower']:.1%} - {m['recall_ci_upper']:.1%})")
    print(f"Precision: {m['precision']:.1%}")
    print(f"F2 Score: {m['f2']:.1%}")
    print(f"\nDeployment Decision: {dep['decision']}")

    if not dep["meets_threshold"]:
        sys.exit(1)


if __name__ == "__main__":
    main()
```
  </action>
  <verify>
```bash
cd "/Users/joshpankin/My Drive/10-19 Projects/12 Development & AI Projects/12.09 Pediatric_Handoff_PHI_Remover"
python tests/run_validation.py --help
```
  </verify>
  <done>
- run_validation.py executes without syntax errors
- --help shows expected arguments (--input, --output, --report, --n-bootstrap)
- Script orchestrates evaluation, CI calculation, and error taxonomy
  </done>
</task>

<task type="auto">
  <name>Task 2: Run validation and generate compliance report</name>
  <files>.planning/phases/05-validation-compliance/VALIDATION_REPORT.md</files>
  <action>
Execute validation on synthetic dataset (as proxy for real data):

```bash
cd "/Users/joshpankin/My Drive/10-19 Projects/12 Development & AI Projects/12.09 Pediatric_Handoff_PHI_Remover"

# Run validation with combined dataset (standard + adversarial)
python tests/run_validation.py \
  --input tests/synthetic_handoffs.json \
  --output .planning/phases/05-validation-compliance/validation_results.json \
  --report .planning/phases/05-validation-compliance/VALIDATION_REPORT.md \
  --n-bootstrap 10000
```

Review the output and verify:
1. Recall CI is calculated
2. Error taxonomy is populated
3. Deployment decision is rendered

If recall CI lower bound is >= 95%, the validation passes.
If recall CI lower bound is < 95%, document the gap and specific failure modes.
  </action>
  <verify>
```bash
cd "/Users/joshpankin/My Drive/10-19 Projects/12 Development & AI Projects/12.09 Pediatric_Handoff_PHI_Remover"
cat .planning/phases/05-validation-compliance/VALIDATION_REPORT.md | head -50
cat .planning/phases/05-validation-compliance/validation_results.json | python -c "import json,sys; d=json.load(sys.stdin); print(f'Recall: {d[\"metrics\"][\"recall\"]:.1%}, CI: [{d[\"metrics\"][\"recall_ci_lower\"]:.1%}, {d[\"metrics\"][\"recall_ci_upper\"]:.1%}]')"
```
  </verify>
  <done>
- VALIDATION_REPORT.md exists with Executive Summary table
- validation_results.json contains metrics with CI bounds
- Deployment decision documented (DEPLOY or RETURN_TO_PHASE_4)
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete validation pipeline and compliance report:
1. Validation runner script (tests/run_validation.py)
2. VALIDATION_REPORT.md with metrics, CI, error taxonomy
3. Deployment readiness assessment
  </what-built>
  <how-to-verify>
1. Review VALIDATION_REPORT.md in the planning directory
2. Check if recall CI lower bound meets 95% threshold
3. Review error taxonomy to understand failure patterns
4. Confirm deployment decision aligns with clinical safety requirements

Commands to run:
```bash
cd "/Users/joshpankin/My Drive/10-19 Projects/12 Development & AI Projects/12.09 Pediatric_Handoff_PHI_Remover"

# View the full report
cat .planning/phases/05-validation-compliance/VALIDATION_REPORT.md

# Check JSON metrics
cat .planning/phases/05-validation-compliance/validation_results.json | python -m json.tool | head -40
```
  </how-to-verify>
  <resume-signal>Type "approved" if validation results are acceptable, or describe concerns/issues</resume-signal>
</task>

</tasks>

<verification>
1. run_validation.py executes without errors
2. VALIDATION_REPORT.md contains required sections
3. Recall CI is calculated and displayed
4. Deployment decision is documented

```bash
cd "/Users/joshpankin/My Drive/10-19 Projects/12 Development & AI Projects/12.09 Pediatric_Handoff_PHI_Remover"

# Verify report structure
grep -E "^## " .planning/phases/05-validation-compliance/VALIDATION_REPORT.md

# Verify metrics present
grep -E "Recall|Precision|F2" .planning/phases/05-validation-compliance/VALIDATION_REPORT.md | head -10

# Verify deployment decision
grep -E "Decision:|DEPLOY|RETURN" .planning/phases/05-validation-compliance/VALIDATION_REPORT.md
```
</verification>

<success_criteria>
- [ ] tests/run_validation.py executes without error
- [ ] run_validation() returns dict with metrics, CI, error_taxonomy, deployment_readiness
- [ ] VALIDATION_REPORT.md exists with Executive Summary, Detailed Metrics, Error Taxonomy, Methodology sections
- [ ] validation_results.json exists with all metrics including CI bounds
- [ ] Deployment decision (DEPLOY/RETURN_TO_PHASE_4) documented based on 95% recall threshold
- [ ] Human verification confirms results are acceptable
</success_criteria>

<output>
After completion, create `.planning/phases/05-validation-compliance/05-03-SUMMARY.md`
</output>
