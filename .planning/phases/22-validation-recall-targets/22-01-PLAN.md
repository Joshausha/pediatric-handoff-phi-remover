---
phase: 22-validation-recall-targets
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/integration/test_phase22_validation.py
  - tests/baselines/phase22_targets.json
autonomous: true

must_haves:
  truths:
    - "ROOM recall meets 55% interim target (achieved 95.6%)"
    - "PHONE_NUMBER recall meets 90% target (achieved 100%)"
    - "LOCATION recall meets 40% floor (achieved 44.2%)"
    - "MRN recall meets 85% target"
    - "Overall weighted recall shows no regression from v2.2"
  artifacts:
    - path: "tests/integration/test_phase22_validation.py"
      provides: "Entity-specific recall threshold enforcement"
      contains: "TestPhase22Validation"
    - path: "tests/baselines/phase22_targets.json"
      provides: "Entity-specific recall targets for regression detection"
      contains: "ROOM"
  key_links:
    - from: "tests/integration/test_phase22_validation.py"
      to: "tests/run_validation.py"
      via: "run_validation import"
      pattern: "from tests.run_validation import run_validation"
    - from: "tests/integration/test_phase22_validation.py"
      to: "tests/baselines/phase22_targets.json"
      via: "JSON baseline load"
      pattern: "phase22_targets.json"
---

<objective>
Create comprehensive entity-specific recall threshold tests for Phase 22 milestone validation.

Purpose: Enforce recall targets from Phases 17-21 via pytest integration tests. Each entity type (ROOM, PHONE_NUMBER, LOCATION, MRN) has specific recall thresholds that must pass for v2.3 milestone completion.

Output: New integration test file with entity-specific threshold enforcement and JSON baseline for regression detection.
</objective>

<execution_context>
@/Users/joshpankin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/joshpankin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/22-validation-recall-targets/22-RESEARCH.md
@tests/integration/test_full_evaluation.py
@tests/integration/test_regression.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create entity-specific recall threshold tests</name>
  <files>tests/integration/test_phase22_validation.py</files>
  <action>
Create `tests/integration/test_phase22_validation.py` with:

```python
"""
Phase 22: Entity-Specific Recall Target Validation

Validates all recall targets from Phases 17-21:
- ROOM: >=55% (Phase 17 interim target, achieved 95.6%)
- PHONE_NUMBER: >=90% (Phase 20 target, achieved 100%)
- LOCATION: >=40% (Phase 21 revised floor, achieved 44.2%)
- MEDICAL_RECORD_NUMBER: >=85% (Phase 22 target)

This module uses module-scoped fixtures to run expensive validation once.
All tests are read-only consumers of the shared validation results.
"""

import json
import pytest
from pathlib import Path
from tests.run_validation import run_validation


@pytest.fixture(scope="module")
def validation_results():
    """Run validation once for all Phase 22 tests."""
    return run_validation(
        input_path=Path("tests/synthetic_handoffs.json"),
        n_bootstrap=1000,
        verbose=False,
    )


def get_entity_recall(entity_stats: dict, entity_type: str) -> float:
    """Calculate recall for a specific entity type."""
    if entity_type not in entity_stats:
        return 0.0
    stats = entity_stats[entity_type]
    tp = stats.get("tp", 0)
    fn = stats.get("fn", 0)
    if tp + fn == 0:
        return 0.0
    return tp / (tp + fn)


class TestPhase22EntityRecall:
    """Entity-specific recall threshold enforcement."""

    def test_room_recall_target(self, validation_results):
        """ROOM recall should meet Phase 17 interim target of 55%.

        Phase 17 achieved 95.6% with number-only lookbehind patterns.
        This test enforces the documented interim target.
        """
        entity_stats = validation_results["metrics"]["entity_stats"]
        recall = get_entity_recall(entity_stats, "ROOM")

        assert recall >= 0.55, (
            f"REGRESSION: ROOM recall {recall:.1%} below 55% interim target. "
            f"Phase 17 achieved 95.6% - investigate pattern regression. "
            f"Stats: {entity_stats.get('ROOM', {})}"
        )

    def test_phone_recall_target(self, validation_results):
        """PHONE_NUMBER recall should meet Phase 20 target of 90%.

        Phase 20 achieved 100% with leniency=0 PhoneRecognizer.
        """
        entity_stats = validation_results["metrics"]["entity_stats"]
        recall = get_entity_recall(entity_stats, "PHONE_NUMBER")

        assert recall >= 0.90, (
            f"REGRESSION: PHONE_NUMBER recall {recall:.1%} below 90% target. "
            f"Phase 20 achieved 100% - check PhoneRecognizer leniency. "
            f"Stats: {entity_stats.get('PHONE_NUMBER', {})}"
        )

    def test_location_recall_floor(self, validation_results):
        """LOCATION recall should meet Phase 21 revised floor of 40%.

        Phase 21 documented pattern-based approach limit at 44.2%.
        Original 60% target was revised after discovering inherent limits.
        17 custom patterns added +24pp improvement over 20% spaCy baseline.
        """
        entity_stats = validation_results["metrics"]["entity_stats"]
        recall = get_entity_recall(entity_stats, "LOCATION")

        assert recall >= 0.40, (
            f"REGRESSION: LOCATION recall {recall:.1%} below 40% floor. "
            f"Phase 21 achieved 44.2% with 17 patterns (pattern-based limit). "
            f"Stats: {entity_stats.get('LOCATION', {})}"
        )

    def test_mrn_recall_target(self, validation_results):
        """MEDICAL_RECORD_NUMBER recall should meet 85% target.

        MRN patterns include hash notation (#12345678) from Phase 5.
        """
        entity_stats = validation_results["metrics"]["entity_stats"]
        recall = get_entity_recall(entity_stats, "MEDICAL_RECORD_NUMBER")

        # Skip if no MRN entities in dataset
        if "MEDICAL_RECORD_NUMBER" not in entity_stats:
            pytest.skip("No MRN entities in validation dataset")

        mrn_stats = entity_stats["MEDICAL_RECORD_NUMBER"]
        if mrn_stats.get("tp", 0) + mrn_stats.get("fn", 0) < 5:
            pytest.skip(f"Insufficient MRN samples: {mrn_stats}")

        assert recall >= 0.85, (
            f"REGRESSION: MRN recall {recall:.1%} below 85% target. "
            f"Stats: {mrn_stats}"
        )


class TestPhase22WeightedMetrics:
    """Overall weighted recall validation."""

    def test_weighted_recall_no_regression(self, validation_results):
        """Weighted recalls should not regress from v2.2 baseline.

        v2.2 baseline:
        - Frequency-weighted: 97.37%
        - Risk-weighted: 91.37%
        """
        metrics = validation_results["metrics"]

        # v2.2 baselines from regression.json
        baseline_freq = 0.9737
        baseline_risk = 0.9137

        current_freq = metrics["freq_weighted_recall"]
        current_risk = metrics["risk_weighted_recall"]

        # Allow 1% tolerance for bootstrap variation
        assert current_freq >= baseline_freq - 0.01, (
            f"REGRESSION: Frequency-weighted recall dropped "
            f"{baseline_freq:.1%} -> {current_freq:.1%}"
        )
        assert current_risk >= baseline_risk - 0.01, (
            f"REGRESSION: Risk-weighted recall dropped "
            f"{baseline_risk:.1%} -> {current_risk:.1%}"
        )

    def test_hipaa_floor_maintained(self, validation_results):
        """Unweighted recall floor (85%) must be maintained.

        This is the HIPAA safety requirement - zero-weight entities
        are invisible in weighted metrics but still matter.
        """
        recall = validation_results["metrics"]["recall"]

        assert recall >= 0.85, (
            f"CRITICAL: Unweighted recall {recall:.1%} below 85% HIPAA floor. "
            f"Weighted metrics cannot replace unweighted recall. "
            f"Zero-weight entities (EMAIL, PEDIATRIC_AGE) still matter."
        )


class TestPhase22PatternLimits:
    """Document pattern-based approach limits for each entity."""

    def test_document_room_ceiling(self, validation_results):
        """ROOM achieved 98% - pattern-based approach ceiling reached."""
        entity_stats = validation_results["metrics"]["entity_stats"]
        recall = get_entity_recall(entity_stats, "ROOM")

        # Just document - not enforcing ceiling, just measuring
        print(f"\n[Phase 22] ROOM recall: {recall:.1%}")
        print(f"  Target: >=55% (interim), Achieved: {recall:.1%}")
        print(f"  Ceiling: ~98% (number-only lookbehind patterns)")

    def test_document_location_limit(self, validation_results):
        """LOCATION achieved 44.2% - pattern-based approach limit reached.

        Further improvement requires:
        - Geographic NER (spaCy GPE, LOCATION entities)
        - Gazetteers (city/hospital name databases)
        """
        entity_stats = validation_results["metrics"]["entity_stats"]
        recall = get_entity_recall(entity_stats, "LOCATION")

        print(f"\n[Phase 22] LOCATION recall: {recall:.1%}")
        print(f"  Original target: >=60%")
        print(f"  Revised floor: >=40% (pattern-based limit)")
        print(f"  spaCy baseline: 20%")
        print(f"  Custom patterns: +{(recall - 0.20)*100:.1f}pp")
        print(f"  Next steps: Geographic NER or gazetteers")
```

Key design decisions:
- Module-scoped fixture runs expensive validation once
- Each entity has its own test with specific threshold from Phase 17-21
- Error messages explain what achieved and how to debug
- Pattern limits documented in test docstrings
- pytest.skip for insufficient samples (not failure)
  </action>
  <verify>
Run `pytest tests/integration/test_phase22_validation.py -v` - all tests should pass

Specifically verify:
- TestPhase22EntityRecall: 4 tests (ROOM, PHONE, LOCATION, MRN)
- TestPhase22WeightedMetrics: 2 tests (weighted regression, HIPAA floor)
- TestPhase22PatternLimits: 2 tests (documentation tests)
  </verify>
  <done>
8 Phase 22 validation tests created enforcing all entity-specific recall targets
  </done>
</task>

<task type="auto">
  <name>Task 2: Create entity-specific baseline file</name>
  <files>tests/baselines/phase22_targets.json</files>
  <action>
Create `tests/baselines/phase22_targets.json` with documented targets:

```json
{
  "description": "Phase 22 entity-specific recall targets from Phases 17-21",
  "created": "2026-01-31",
  "targets": {
    "ROOM": {
      "floor": 0.55,
      "achieved": 0.956,
      "phase": 17,
      "notes": "Number-only lookbehind patterns, interim target revised from 80%"
    },
    "PHONE_NUMBER": {
      "floor": 0.90,
      "achieved": 1.00,
      "phase": 20,
      "notes": "PhoneRecognizer with leniency=0"
    },
    "LOCATION": {
      "floor": 0.40,
      "achieved": 0.442,
      "phase": 21,
      "notes": "Pattern-based limit, original 60% target revised. 17 patterns added +24pp."
    },
    "MEDICAL_RECORD_NUMBER": {
      "floor": 0.85,
      "achieved": null,
      "phase": 22,
      "notes": "Phase 5 patterns, validated in Phase 22"
    }
  },
  "weighted_baselines": {
    "freq_weighted_recall": 0.9737,
    "risk_weighted_recall": 0.9137,
    "source": "regression.json from Phase 16"
  }
}
```

This file:
- Documents targets in version control
- Separates Phase 22 targets from overall regression baseline
- Includes notes for each entity explaining source and rationale
- Allows baseline comparison without duplicating regression.json
  </action>
  <verify>
File exists and is valid JSON: `python -c "import json; json.load(open('tests/baselines/phase22_targets.json'))"`

Targets match roadmap:
- ROOM >= 55%
- PHONE_NUMBER >= 90%
- LOCATION >= 40%
- MRN >= 85%
  </verify>
  <done>
Entity-specific baseline created documenting Phase 17-21 targets
  </done>
</task>

<task type="auto">
  <name>Task 3: Run validation and verify all targets met</name>
  <files>tests/integration/test_phase22_validation.py</files>
  <action>
1. Run full Phase 22 validation suite:
   ```bash
   pytest tests/integration/test_phase22_validation.py -v --tb=short
   ```

2. Run full integration test suite to confirm no regressions:
   ```bash
   pytest tests/integration/ -v --tb=short
   ```

3. Capture validation output for summary:
   - Note any xfailed tests with reasons
   - Document actual recall values vs targets

4. If any tests fail unexpectedly:
   - Check entity_stats output in error message
   - Verify patterns still registered in deidentification.py
   - Compare with Phase 21 verification report metrics
  </action>
  <verify>
All Phase 22 tests pass:
- TestPhase22EntityRecall: 4/4 pass (or skip for MRN if insufficient samples)
- TestPhase22WeightedMetrics: 2/2 pass
- TestPhase22PatternLimits: 2/2 pass (documentation tests)

Integration test count: 8 new + existing (test_full_evaluation: 4, test_regression: 2)
  </verify>
  <done>
Phase 22 validation complete - all entity recall targets verified
  </done>
</task>

</tasks>

<verification>
1. tests/integration/test_phase22_validation.py exists with 8 tests
2. tests/baselines/phase22_targets.json exists with all 4 entity targets
3. All Phase 22 tests pass (or skip appropriately)
4. Existing integration tests still pass (no regression)
5. Module-scoped fixture pattern used (not function-scoped)
</verification>

<success_criteria>
- [ ] test_phase22_validation.py created with entity-specific tests
- [ ] phase22_targets.json created with documented targets
- [ ] ROOM recall >= 55% verified
- [ ] PHONE_NUMBER recall >= 90% verified
- [ ] LOCATION recall >= 40% verified
- [ ] MRN recall >= 85% verified (or skipped if insufficient samples)
- [ ] Weighted recall no regression from v2.2
- [ ] HIPAA floor (85%) maintained
- [ ] No regressions on existing integration tests
</success_criteria>

<output>
After completion, create `.planning/phases/22-validation-recall-targets/22-01-SUMMARY.md`
</output>
