---
phase: 01-baseline-measurement
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/evaluate_presidio.py
autonomous: true

must_haves:
  truths:
    - "Running evaluate_presidio.py outputs F2 score alongside F1"
    - "Per-entity breakdown includes F2 column"
    - "JSON output includes F2 metric and confusion matrix"
    - "--export-confusion-matrix flag exports detailed breakdown"
  artifacts:
    - path: "tests/evaluate_presidio.py"
      provides: "Enhanced evaluation with F2 score and confusion matrix"
      contains: "def f2"
  key_links:
    - from: "tests/evaluate_presidio.py"
      to: "EvaluationMetrics.f2"
      via: "property method"
      pattern: "@property.*def f2"
---

<objective>
Enhance the Presidio evaluation script with F2 score calculation and confusion matrix export.

Purpose: F2 score (recall-weighted) is the appropriate optimization target for PHI detection because false negatives (PHI leaks) are more dangerous than false positives (over-redaction). The confusion matrix enables threshold calibration in Phase 2.

Output: Updated `tests/evaluate_presidio.py` with F2 metrics in all output modes (text report, JSON, per-entity breakdown).
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@tests/evaluate_presidio.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add F2 score property to EvaluationMetrics</name>
  <files>tests/evaluate_presidio.py</files>
  <action>
Add an `f2` property to the `EvaluationMetrics` dataclass:

```python
@property
def f2(self) -> float:
    """F2 score (recall-weighted): beta=2 emphasizes recall 2x precision.

    F2 = (1 + beta^2) * (precision * recall) / (beta^2 * precision + recall)
    where beta = 2
    """
    beta = 2.0
    if self.precision + self.recall == 0:
        return 0.0
    return (1 + beta**2) * (self.precision * self.recall) / (beta**2 * self.precision + self.recall)
```

Place this property after the existing `f1` property (around line 68-72).
  </action>
  <verify>
Run: `cd "/Users/joshpankin/My Drive/10-19 Projects/12 Development & AI Projects/12.09 Pediatric_Handoff_PHI_Remover" && python -c "from tests.evaluate_presidio import EvaluationMetrics; m = EvaluationMetrics(total_expected=100, true_positives=80, false_negatives=20, false_positives=10); print(f'F2: {m.f2:.3f}')"` should print F2 value.
  </verify>
  <done>EvaluationMetrics.f2 property exists and returns correct F2 score.</done>
</task>

<task type="auto">
  <name>Task 2: Add F2 to text report and per-entity breakdown</name>
  <files>tests/evaluate_presidio.py</files>
  <action>
1. Update `generate_report` method to include F2 in the overall metrics section:
   - After the line `f"  F1 Score:  {metrics.f1:.1%}"` (around line 298), add:
   - `f"  F2 Score:  {metrics.f2:.1%}  â† PRIMARY METRIC (recall-weighted)"`

2. Update the per-type performance section to include F2:
   - Modify the type_stats dictionary to also track false_positives per type
   - Calculate precision and F2 for each entity type
   - Update the output format from:
     `{status} {phi_type}: {tp}/{total} ({recall:.1%} recall)`
   - To:
     `{status} {phi_type}: P={precision:.0%} R={recall:.0%} F1={f1:.0%} F2={f2:.0%}`

3. To calculate per-entity F2:
   - For each entity type, precision = tp / (tp + fp) if denominator > 0 else 0
   - Then F2 = (5 * precision * recall) / (4 * precision + recall) if denominator > 0 else 0
  </action>
  <verify>
Run: `cd "/Users/joshpankin/My Drive/10-19 Projects/12 Development & AI Projects/12.09 Pediatric_Handoff_PHI_Remover" && python tests/evaluate_presidio.py 2>/dev/null | grep -E "(F2 Score|F2=)"` should show F2 metrics.
  </verify>
  <done>Text report includes F2 score overall and per-entity breakdown shows P/R/F1/F2.</done>
</task>

<task type="auto">
  <name>Task 3: Add F2 and confusion matrix to JSON output</name>
  <files>tests/evaluate_presidio.py</files>
  <action>
1. In the JSON output section of `main()` (around line 399-424), add "f2" to the metrics dict:
   ```python
   "metrics": {
       ...
       "f2": metrics.f2,  # Add this line after f1
       ...
   }
   ```

2. Add a new --export-confusion-matrix CLI argument (after --overlap around line 375):
   ```python
   parser.add_argument(
       "--export-confusion-matrix",
       type=Path,
       help="Export per-entity confusion matrix to JSON file"
   )
   ```

3. Before the JSON output section, if args.export_confusion_matrix is provided, create and export the confusion matrix:
   ```python
   if args.export_confusion_matrix:
       # Build per-entity confusion matrix
       confusion = {}
       for result in results:
           for span in result.true_positives:
               confusion.setdefault(span.entity_type, {"tp": 0, "fn": 0, "fp": 0})
               confusion[span.entity_type]["tp"] += 1
           for span in result.false_negatives:
               confusion.setdefault(span.entity_type, {"tp": 0, "fn": 0, "fp": 0})
               confusion[span.entity_type]["fn"] += 1
           for det in result.false_positives:
               confusion.setdefault(det["entity_type"], {"tp": 0, "fn": 0, "fp": 0})
               confusion[det["entity_type"]]["fp"] += 1

       # Calculate metrics per entity
       for entity_type, counts in confusion.items():
           tp, fn, fp = counts["tp"], counts["fn"], counts["fp"]
           counts["precision"] = tp / (tp + fp) if (tp + fp) > 0 else 0.0
           counts["recall"] = tp / (tp + fn) if (tp + fn) > 0 else 0.0
           p, r = counts["precision"], counts["recall"]
           counts["f1"] = 2 * p * r / (p + r) if (p + r) > 0 else 0.0
           counts["f2"] = 5 * p * r / (4 * p + r) if (4 * p + r) > 0 else 0.0

       args.export_confusion_matrix.parent.mkdir(parents=True, exist_ok=True)
       with open(args.export_confusion_matrix, "w") as f:
           json.dump(confusion, f, indent=2)
       print(f"Confusion matrix exported to {args.export_confusion_matrix}")
   ```
  </action>
  <verify>
1. Run: `cd "/Users/joshpankin/My Drive/10-19 Projects/12 Development & AI Projects/12.09 Pediatric_Handoff_PHI_Remover" && python tests/evaluate_presidio.py --json 2>/dev/null | python -c "import json,sys; d=json.load(sys.stdin); print('f2' in d['metrics'])"` should print True.
2. Run: `cd "/Users/joshpankin/My Drive/10-19 Projects/12 Development & AI Projects/12.09 Pediatric_Handoff_PHI_Remover" && python tests/evaluate_presidio.py --export-confusion-matrix /tmp/confusion.json 2>/dev/null && cat /tmp/confusion.json | python -c "import json,sys; d=json.load(sys.stdin); print(len(d) > 0)"` should print True.
  </verify>
  <done>JSON output includes F2 metric and --export-confusion-matrix exports per-entity breakdown.</done>
</task>

</tasks>

<verification>
Overall verification steps:
1. Run full evaluation: `python tests/evaluate_presidio.py --verbose`
   - Verify F2 score appears in output
   - Verify per-entity breakdown shows P/R/F1/F2 format
2. Test JSON output: `python tests/evaluate_presidio.py --json | jq '.metrics.f2'`
   - Verify F2 value is returned
3. Test confusion matrix export: `python tests/evaluate_presidio.py --export-confusion-matrix confusion.json && cat confusion.json`
   - Verify JSON contains per-entity TP/FN/FP with precision/recall/F1/F2
4. Run existing tests: `pytest tests/test_deidentification.py -v --tb=short`
   - Verify no regressions (existing tests still pass/fail as before)
</verification>

<success_criteria>
1. F2 score property added to EvaluationMetrics class
2. Text report shows F2 overall and per-entity (P/R/F1/F2 format)
3. JSON output includes "f2" field in metrics
4. --export-confusion-matrix CLI flag works and exports per-entity breakdown
5. No regressions in existing functionality
</success_criteria>

<output>
After completion, create `.planning/phases/01-baseline-measurement/01-01-SUMMARY.md`
</output>
