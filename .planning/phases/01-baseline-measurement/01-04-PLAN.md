---
phase: 01-baseline-measurement
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/check_thresholds.py
  - .planning/phases/01-baseline-measurement/CICD_STRATEGY.md
autonomous: true

must_haves:
  truths:
    - "Threshold check script exists and is executable"
    - "Script exits 1 if recall < 95% or precision < 70%"
    - "CI/CD strategy document outlines GitHub Actions workflow"
    - "Strategy marked as v2 implementation (not blocking Phase 1)"
  artifacts:
    - path: "tests/check_thresholds.py"
      provides: "Standalone threshold validation script"
      contains: "RECALL_MIN = 0.95"
      min_lines: 30
    - path: ".planning/phases/01-baseline-measurement/CICD_STRATEGY.md"
      provides: "CI/CD integration documentation"
      contains: "GitHub Actions"
      min_lines: 50
  key_links:
    - from: "tests/check_thresholds.py"
      to: "tests/evaluate_presidio.py"
      via: "consumes JSON output"
      pattern: "--json"
---

<objective>
Document the CI/CD integration strategy and create a threshold check script skeleton.

Purpose: While full CI/CD implementation is deferred to v2, having the strategy documented and a working threshold check script prepares for future integration. The script provides immediate value for manual pre-commit checks.

Output: Executable `tests/check_thresholds.py` script and comprehensive `CICD_STRATEGY.md` documenting the GitHub Actions workflow approach.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@tests/evaluate_presidio.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create threshold check script</name>
  <files>tests/check_thresholds.py</files>
  <action>
Create a standalone script that validates PHI detection metrics against safety thresholds:

```python
#!/usr/bin/env python3
"""
Check PHI detection metrics against safety thresholds.

This script validates that the PHI detection system meets minimum
recall (for safety) and precision (for clinical utility) thresholds.

Usage:
    python tests/evaluate_presidio.py --json | python tests/check_thresholds.py
    python tests/check_thresholds.py metrics.json
    python tests/check_thresholds.py --recall-min 0.90 --precision-min 0.65 metrics.json

Exit codes:
    0: All thresholds passed
    1: One or more thresholds failed
"""

import argparse
import json
import sys
from pathlib import Path


# Default safety thresholds
DEFAULT_RECALL_MIN = 0.95      # HIPAA compliance: catch >95% of PHI
DEFAULT_PRECISION_MIN = 0.70   # Clinical utility: don't over-redact
DEFAULT_F2_MIN = 0.90          # Recall-weighted: balance safety and utility


def check_thresholds(
    metrics: dict,
    recall_min: float = DEFAULT_RECALL_MIN,
    precision_min: float = DEFAULT_PRECISION_MIN,
    f2_min: float = DEFAULT_F2_MIN,
) -> tuple[bool, list[str]]:
    """
    Check metrics against thresholds.

    Args:
        metrics: Dict with 'recall', 'precision', 'f2' keys
        recall_min: Minimum recall threshold
        precision_min: Minimum precision threshold
        f2_min: Minimum F2 score threshold

    Returns:
        Tuple of (all_passed, list of failure messages)
    """
    failures = []

    recall = metrics.get("recall", 0)
    precision = metrics.get("precision", 0)
    f2 = metrics.get("f2", 0)

    if recall < recall_min:
        failures.append(
            f"RECALL: {recall:.1%} < {recall_min:.0%} threshold "
            f"(PHI LEAK RISK: {(1 - recall) * 100:.1f}% of PHI may be missed)"
        )

    if precision < precision_min:
        failures.append(
            f"PRECISION: {precision:.1%} < {precision_min:.0%} threshold "
            f"(OVER-REDACTION: {(1 - precision) * 100:.1f}% false positives)"
        )

    if f2 < f2_min:
        failures.append(
            f"F2 SCORE: {f2:.1%} < {f2_min:.0%} threshold "
            f"(OVERALL: recall-weighted performance below target)"
        )

    return len(failures) == 0, failures


def main():
    parser = argparse.ArgumentParser(
        description="Check PHI detection metrics against safety thresholds",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    parser.add_argument(
        "input",
        nargs="?",
        type=Path,
        help="JSON metrics file (or reads from stdin if not provided)",
    )
    parser.add_argument(
        "--recall-min",
        type=float,
        default=DEFAULT_RECALL_MIN,
        help=f"Minimum recall threshold (default: {DEFAULT_RECALL_MIN:.0%})",
    )
    parser.add_argument(
        "--precision-min",
        type=float,
        default=DEFAULT_PRECISION_MIN,
        help=f"Minimum precision threshold (default: {DEFAULT_PRECISION_MIN:.0%})",
    )
    parser.add_argument(
        "--f2-min",
        type=float,
        default=DEFAULT_F2_MIN,
        help=f"Minimum F2 score threshold (default: {DEFAULT_F2_MIN:.0%})",
    )
    parser.add_argument(
        "--quiet", "-q",
        action="store_true",
        help="Only output failures (no success messages)",
    )

    args = parser.parse_args()

    # Load metrics from file or stdin
    try:
        if args.input:
            with open(args.input) as f:
                data = json.load(f)
        else:
            data = json.load(sys.stdin)
    except json.JSONDecodeError as e:
        print(f"Error: Invalid JSON input: {e}", file=sys.stderr)
        sys.exit(1)
    except FileNotFoundError:
        print(f"Error: File not found: {args.input}", file=sys.stderr)
        sys.exit(1)

    # Extract metrics (handle both direct and nested formats)
    metrics = data.get("metrics", data)

    # Check thresholds
    passed, failures = check_thresholds(
        metrics,
        recall_min=args.recall_min,
        precision_min=args.precision_min,
        f2_min=args.f2_min,
    )

    # Output results
    if failures:
        print("=" * 60)
        print("PHI DETECTION THRESHOLD CHECK: FAILED")
        print("=" * 60)
        for failure in failures:
            print(f"  {failure}")
        print()
        print("Action required: Improve PHI detection before deployment.")
        print("=" * 60)
        sys.exit(1)
    else:
        if not args.quiet:
            print("=" * 60)
            print("PHI DETECTION THRESHOLD CHECK: PASSED")
            print("=" * 60)
            print(f"  Recall:    {metrics.get('recall', 0):.1%} >= {args.recall_min:.0%}")
            print(f"  Precision: {metrics.get('precision', 0):.1%} >= {args.precision_min:.0%}")
            print(f"  F2 Score:  {metrics.get('f2', 0):.1%} >= {args.f2_min:.0%}")
            print("=" * 60)
        sys.exit(0)


if __name__ == "__main__":
    main()
```

Make the script executable:
```bash
chmod +x tests/check_thresholds.py
```
  </action>
  <verify>
1. Run: `python tests/check_thresholds.py --help` shows usage
2. Run: `echo '{"metrics": {"recall": 0.99, "precision": 0.80, "f2": 0.95}}' | python tests/check_thresholds.py` exits 0
3. Run: `echo '{"metrics": {"recall": 0.70, "precision": 0.80, "f2": 0.75}}' | python tests/check_thresholds.py` exits 1
  </verify>
  <done>tests/check_thresholds.py created and is executable with proper threshold validation.</done>
</task>

<task type="auto">
  <name>Task 2: Create CI/CD strategy document</name>
  <files>.planning/phases/01-baseline-measurement/CICD_STRATEGY.md</files>
  <action>
Create a comprehensive CI/CD strategy document:

```markdown
# CI/CD Integration Strategy for PHI Detection Metrics

**Status**: v2 Implementation (documented for future, not blocking Phase 1)
**Document Date**: [Current date]
**Author**: Automated planning

## Overview

This document outlines the strategy for integrating PHI detection metrics into the CI/CD pipeline. The goal is to automatically detect regressions in PHI detection quality before they reach production.

## Trigger Strategy

### When to Run PHI Detection Evaluation

The evaluation workflow should trigger on:
- Pull requests that modify PHI-related code
- Commits to main branch (post-merge validation)
- Scheduled nightly runs (detect drift)

### File Patterns to Watch

```yaml
paths:
  - 'app/recognizers/**'
  - 'app/deidentification.py'
  - 'app/config.py'
  - 'tests/handoff_templates.py'
  - 'tests/medical_providers.py'
```

## GitHub Actions Workflow

### Proposed Workflow: `.github/workflows/phi-detection-metrics.yml`

```yaml
name: PHI Detection Metrics

on:
  pull_request:
    paths:
      - 'app/recognizers/**'
      - 'app/deidentification.py'
      - 'app/config.py'
  push:
    branches: [main]
    paths:
      - 'app/recognizers/**'
      - 'app/deidentification.py'
      - 'app/config.py'
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:  # Manual trigger

jobs:
  evaluate-phi-detection:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          python -m spacy download en_core_web_lg

      - name: Generate synthetic dataset
        run: |
          python tests/generate_test_data.py --samples 500 --seed 42

      - name: Run PHI detection evaluation
        run: |
          python tests/evaluate_presidio.py --json > metrics.json
          python tests/evaluate_presidio.py --export-confusion-matrix confusion.json

      - name: Check safety thresholds
        run: |
          python tests/check_thresholds.py metrics.json

      - name: Upload metrics artifact
        uses: actions/upload-artifact@v4
        with:
          name: phi-metrics
          path: |
            metrics.json
            confusion.json
          retention-days: 30

      - name: Comment PR with metrics
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const metrics = JSON.parse(fs.readFileSync('metrics.json', 'utf8'));
            const m = metrics.metrics;

            const body = `## PHI Detection Metrics

            | Metric | Value | Target | Status |
            |--------|-------|--------|--------|
            | Recall | ${(m.recall * 100).toFixed(1)}% | >95% | ${m.recall >= 0.95 ? '' : ''} |
            | Precision | ${(m.precision * 100).toFixed(1)}% | >70% | ${m.precision >= 0.70 ? '' : ''} |
            | F2 Score | ${(m.f2 * 100).toFixed(1)}% | >90% | ${m.f2 >= 0.90 ? '' : ''} |

            <details>
            <summary>Per-Entity Performance</summary>

            See [metrics.json](${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID}) artifact for details.
            </details>`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });
```

## Threshold Configuration

### Default Thresholds (Safety-First)

| Metric | Threshold | Rationale |
|--------|-----------|-----------|
| Recall | >= 95% | HIPAA compliance: minimize PHI leaks |
| Precision | >= 70% | Clinical utility: don't over-redact |
| F2 Score | >= 90% | Recall-weighted balance |

### Override for Development

During active development (Phases 2-4), thresholds may need adjustment:

```bash
# Temporarily lower thresholds for work-in-progress
python tests/check_thresholds.py --recall-min 0.80 --precision-min 0.60 metrics.json
```

## Integration with Existing CI

### Current Workflow: `.github/workflows/test.yml`

The PHI metrics workflow should run AFTER the existing test workflow passes. This ensures:
1. Basic tests pass first (faster feedback)
2. PHI evaluation only runs on valid code

Recommend adding:
```yaml
jobs:
  test:
    # Existing test job

  phi-metrics:
    needs: test  # Only run if tests pass
    # PHI evaluation job
```

## Metrics Tracking Over Time

### Recommended: Metrics Dashboard

For v2+, consider:
1. **Grafana/Prometheus**: Time-series metrics visualization
2. **GitHub Actions Summary**: Per-run metrics in workflow summary
3. **PR Comments**: Automatic metric comparison (current vs baseline)

### Baseline Comparison Strategy

```bash
# Compare current metrics to baseline
python scripts/compare_metrics.py baseline_metrics.json current_metrics.json
```

Output:
```
Recall:    77.9% -> 82.3% (+4.4%)
Precision: 66.3% -> 71.5% (+5.2%)
F2 Score:  71.7% -> 76.2% (+4.5%)
```

## Implementation Timeline

| Phase | Action | Priority |
|-------|--------|----------|
| v1 (Current) | Document strategy, create check_thresholds.py | DONE |
| v2 (Post-Phase 5) | Implement GitHub Actions workflow | High |
| v3 (Future) | Add metrics dashboard, baseline comparison | Medium |
| v4 (Future) | Integrate with deployment gates | Low |

## Security Considerations

### No PHI in CI/CD

- Synthetic datasets only (no real patient data)
- Dataset generation is deterministic (seed=42)
- No PHI should ever be committed to the repository

### Secrets Management

If integrating with external services (e.g., metrics dashboard):
- Use GitHub Secrets for API keys
- Never log sensitive data in workflow output

## Manual Validation Commands

For local development, run these commands before committing:

```bash
# Quick check (30 seconds)
python tests/evaluate_presidio.py --json | python tests/check_thresholds.py

# Full evaluation (2 minutes)
python tests/evaluate_presidio.py --verbose

# Compare with baseline
python tests/evaluate_presidio.py --json > current.json
diff <(jq '.metrics' baseline.json) <(jq '.metrics' current.json)
```

## Failure Handling

### If Thresholds Fail

1. **PR blocked**: Cannot merge until thresholds pass
2. **Notification**: Team notified via PR comment
3. **Investigation**: Check confusion matrix for specific failures
4. **Action**: Either fix the issue or adjust thresholds with justification

### Threshold Adjustment Process

If thresholds need to change:
1. Document rationale in PR description
2. Update `tests/check_thresholds.py` default values
3. Update this document's threshold table
4. Require approval from project owner

---

## Summary

This CI/CD strategy ensures PHI detection quality is automatically validated before changes reach production. The two-tier approach (fast threshold check + detailed evaluation) provides quick feedback while maintaining comprehensive metrics tracking.

**Next Steps** (v2 Implementation):
1. Create `.github/workflows/phi-detection-metrics.yml`
2. Test on a feature branch
3. Enable PR blocking on threshold failures
4. Add baseline comparison to PR comments

---
*Strategy document created during Phase 1: Baseline Measurement*
*Implementation deferred to v2 (post-Phase 5 validation)*
```
  </action>
  <verify>
1. File exists at `.planning/phases/01-baseline-measurement/CICD_STRATEGY.md`
2. Contains GitHub Actions YAML example
3. Contains threshold configuration table
4. Marked as "v2 Implementation"
  </verify>
  <done>CICD_STRATEGY.md created with comprehensive integration documentation.</done>
</task>

</tasks>

<verification>
Overall verification steps:
1. `tests/check_thresholds.py` exists and is executable
2. Script validates correctly: passing metrics exit 0, failing metrics exit 1
3. Script supports stdin and file input
4. CICD_STRATEGY.md exists with complete workflow documentation
5. Strategy document clearly marked as v2 (not blocking Phase 1)
</verification>

<success_criteria>
1. check_thresholds.py created with configurable threshold validation
2. Script works with both stdin (piped) and file input
3. Exit codes are correct (0 for pass, 1 for fail)
4. CICD_STRATEGY.md documents GitHub Actions workflow
5. Strategy includes threshold rationale and override instructions
6. Document marked as v2 implementation
</success_criteria>

<output>
After completion, create `.planning/phases/01-baseline-measurement/01-04-SUMMARY.md`
</output>
