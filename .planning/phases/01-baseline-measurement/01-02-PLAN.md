---
phase: 01-baseline-measurement
plan: 02
type: execute
wave: 3
depends_on: ["01-01", "01-03"]
files_modified:
  - .planning/phases/01-baseline-measurement/BASELINE_METRICS.md
autonomous: true

must_haves:
  truths:
    - "Baseline metrics document exists with timestamp"
    - "All 8 entity types have documented recall/precision/F1/F2 scores for both standard and adversarial datasets"
    - "Known limitations are explicitly documented including MEAS-02 deferral"
    - "Current test failures are catalogued"
    - "Adversarial dataset gaps documented with priority for Phase 4"
  artifacts:
    - path: ".planning/phases/01-baseline-measurement/BASELINE_METRICS.md"
      provides: "Timestamped baseline before any improvements"
      contains: "Adversarial Dataset Evaluation"
      min_lines: 100
  key_links:
    - from: ".planning/phases/01-baseline-measurement/BASELINE_METRICS.md"
      to: "tests/evaluate_presidio.py"
      via: "metrics captured from evaluation run"
      pattern: "F2 Score"
    - from: ".planning/phases/01-baseline-measurement/BASELINE_METRICS.md"
      to: "tests/adversarial_handoffs.json"
      via: "adversarial evaluation results"
      pattern: "Adversarial"
---

<objective>
Document the current baseline performance metrics before any system improvements.

Purpose: This creates a defensible "before" snapshot for the research project. All subsequent improvements will be measured against this baseline. The document provides evidence for the research poster and QI project documentation.

Output: Comprehensive BASELINE_METRICS.md with timestamped metrics, per-entity performance, known limitations, test failure analysis, and adversarial dataset evaluation results.

**MEAS-02 Deferral Note**: Human-annotated gold standard dataset requirement is deferred to Phase 5 (validation phase). Phase 1 uses synthetic datasets (standard + adversarial) as the gold standard. This deferral is intentional - real transcript annotation requires IRB coordination and is validation-phase work, not baseline-phase work.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-baseline-measurement/01-01-SUMMARY.md
@.planning/phases/01-baseline-measurement/01-03-SUMMARY.md
@tests/evaluate_presidio.py
@tests/adversarial_handoffs.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run evaluation and capture metrics</name>
  <files>None (data collection)</files>
  <action>
Run the enhanced evaluation script to capture current metrics:

1. Run text report for human-readable output:
   ```bash
   cd "/Users/joshpankin/My Drive/10-19 Projects/12 Development & AI Projects/12.09 Pediatric_Handoff_PHI_Remover"
   python tests/evaluate_presidio.py --verbose 2>&1 | tee /tmp/baseline_report.txt
   ```

2. Run JSON output for precise numbers:
   ```bash
   python tests/evaluate_presidio.py --json 2>/dev/null > /tmp/baseline_metrics.json
   ```

3. Export confusion matrix for per-entity analysis:
   ```bash
   python tests/evaluate_presidio.py --export-confusion-matrix /tmp/baseline_confusion.json 2>/dev/null
   ```

4. Capture pytest test results:
   ```bash
   pytest tests/test_deidentification.py -v --tb=no 2>&1 | tee /tmp/test_results.txt
   ```

5. Capture system info:
   ```bash
   python -c "import presidio_analyzer; print(f'Presidio: {presidio_analyzer.__version__}')" 2>/dev/null || echo "Presidio version unavailable"
   python -c "import spacy; print(f'spaCy: {spacy.__version__}')" 2>/dev/null || echo "spaCy version unavailable"
   ```

6. Capture adversarial dataset metrics (from Plan 01-03 output):
   ```bash
   # Adversarial dataset evaluation (should already exist from 01-03)
   python tests/evaluate_presidio.py --input tests/adversarial_handoffs.json --json 2>/dev/null > /tmp/adversarial_metrics.json
   python tests/evaluate_presidio.py --input tests/adversarial_handoffs.json --export-confusion-matrix /tmp/adversarial_confusion.json 2>/dev/null
   ```

Record all captured data for use in Task 2.
  </action>
  <verify>
Files exist: `/tmp/baseline_report.txt`, `/tmp/baseline_metrics.json`, `/tmp/baseline_confusion.json`, `/tmp/test_results.txt`, `/tmp/adversarial_metrics.json`, `/tmp/adversarial_confusion.json`
  </verify>
  <done>Raw metrics data captured from evaluation runs.</done>
</task>

<task type="auto">
  <name>Task 2: Create BASELINE_METRICS.md document</name>
  <files>.planning/phases/01-baseline-measurement/BASELINE_METRICS.md</files>
  <action>
Create a comprehensive baseline metrics document with the following structure:

```markdown
# Baseline Metrics: PHI Detection System

**Capture Date**: [Current date/time in YYYY-MM-DD HH:MM:SS format]
**Dataset**: tests/synthetic_handoffs.json (500 samples, seed=42)
**Evaluation Script**: tests/evaluate_presidio.py (overlap threshold: 0.5)

## System Configuration

| Component | Version | Notes |
|-----------|---------|-------|
| Presidio Analyzer | [version] | Microsoft PHI detection engine |
| spaCy Model | en_core_web_lg | NER backbone |
| Detection Threshold | 0.35 | Score threshold for entity detection |
| Validation Threshold | 0.7 | Threshold for de-identification validation |

## Overall Performance

| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| Recall | [from JSON] | >95% | [status] |
| Precision | [from JSON] | >70% | [status] |
| F1 Score | [from JSON] | - | - |
| **F2 Score** | [from JSON] | - | **PRIMARY METRIC** |

## Per-Entity Performance

[Table from confusion matrix with all 8 entity types:]
| Entity Type | TP | FN | FP | Precision | Recall | F1 | F2 | Status |
|-------------|----|----|----|-----------|---------|----|----|----- |
| EMAIL | ... | ... | ... | ... | ... | ... | ... | [status] |
| PERSON | ... | ... | ... | ... | ... | ... | ... | [status] |
| DATE_TIME | ... | ... | ... | ... | ... | ... | ... | [status] |
| PHONE_NUMBER | ... | ... | ... | ... | ... | ... | ... | [status] |
| MEDICAL_RECORD_NUMBER | ... | ... | ... | ... | ... | ... | ... | [status] |
| LOCATION | ... | ... | ... | ... | ... | ... | ... | [status] |
| ROOM | ... | ... | ... | ... | ... | ... | ... | [status] |
| PEDIATRIC_AGE | ... | ... | ... | ... | ... | ... | ... | [status] |

## Critical Gaps Analysis

### Weakest Performers (Recall <80%)
1. **PEDIATRIC_AGE**: [recall]% - [analysis of why]
2. **ROOM**: [recall]% - [analysis of why]
3. **LOCATION**: [recall]% - [analysis of why]

### Root Causes
- Regex lookbehind patterns only catch "Mom Jessica", miss "Jessica is Mom"
- Deny list case sensitivity inconsistent (PERSON case-insensitive, LOCATION exact match)
- Detection threshold (0.35) and validation threshold (0.7) not calibrated

## Test Suite Status

**Tests Passing**: [X]/36
**Tests Failing**: [Y]/36

### Failing Tests
[List each failing test with brief explanation:]
- `test_catches_mrn_with_label`: MRN with label prefix not detected
- `test_minimal_phi_transcript`: Over-redacting clinical abbreviations
- `test_bulk_person_detection`: [X] missed persons in bulk test
- ...

## Adversarial Dataset Evaluation

**Dataset**: tests/adversarial_handoffs.json (100 samples, seed=43)
**Purpose**: Edge cases to stress-test PHI detection patterns

### Adversarial Performance vs Standard

| Metric | Standard (500 samples) | Adversarial (100 samples) | Delta |
|--------|------------------------|---------------------------|-------|
| Recall | [from standard] | [from adversarial] | [diff] |
| Precision | [from standard] | [from adversarial] | [diff] |
| F2 Score | [from standard] | [from adversarial] | [diff] |

### Adversarial Per-Entity Performance

| Entity Type | Standard Recall | Adversarial Recall | Gap | Notes |
|-------------|-----------------|--------------------|----|-------|
| EMAIL | [%] | [%] | [%] | [analysis] |
| PERSON | [%] | [%] | [%] | [analysis] |
| DATE_TIME | [%] | [%] | [%] | [analysis] |
| PHONE_NUMBER | [%] | [%] | [%] | [analysis] |
| MEDICAL_RECORD_NUMBER | [%] | [%] | [%] | [analysis] |
| LOCATION | [%] | [%] | [%] | [analysis] |
| ROOM | [%] | [%] | [%] | [analysis] |
| PEDIATRIC_AGE | [%] | [%] | [%] | [analysis] |

### Edge Case Categories Analysis

| Category | Template Count | Recall | Critical Failures |
|----------|----------------|--------|-------------------|
| Speech Artifacts | [N] | [%] | [specific patterns] |
| Diverse Names | [N] | [%] | [specific patterns] |
| Boundary Edges | [N] | [%] | [specific patterns] |
| Transcription Errors | [N] | [%] | [specific patterns] |
| Compound Patterns | [N] | [%] | [specific patterns] |
| Age Edges | [N] | [%] | [specific patterns] |

### Priority Gaps for Phase 4

1. **[Gap 1]**: [description and impact]
2. **[Gap 2]**: [description and impact]
3. **[Gap 3]**: [description and impact]

## Known Limitations

1. **Synthetic-only validation**: Dataset is programmatically generated, not from real transcripts
2. **Template diversity**: 50 unique templates may not cover all clinical scenarios (addressed by adversarial dataset)
3. **Human annotation deferred**: MEAS-02 (human-annotated gold standard) deferred to Phase 5 for IRB coordination
4. **Limited name diversity**: Faker defaults may not represent diverse patient populations
5. **Threshold not calibrated**: Detection threshold (0.35) set arbitrarily, not data-driven

## Improvement Roadmap

Based on baseline analysis:
1. **Phase 2**: Threshold calibration using precision-recall curves
2. **Phase 3**: Deny list refinement and case normalization
3. **Phase 4**: Pattern improvements for PEDIATRIC_AGE, ROOM, lookbehind edge cases
4. **Phase 5**: Validation on real transcripts

## Raw Data References

- Full evaluation report: Run `python tests/evaluate_presidio.py --verbose`
- JSON metrics: Run `python tests/evaluate_presidio.py --json`
- Confusion matrix: Run `python tests/evaluate_presidio.py --export-confusion-matrix confusion.json`
- Synthetic dataset: `tests/synthetic_handoffs.json`

---
*Baseline captured before any PHI detection improvements.*
*Next step: Phase 2 (Threshold Calibration)*
```

Fill in all values from the captured metrics data.
  </action>
  <verify>
1. File exists at `.planning/phases/01-baseline-measurement/BASELINE_METRICS.md`
2. Contains timestamp in header
3. Contains all 8 entity types in per-entity table
4. Contains "F2 Score" as PRIMARY METRIC
5. Contains "Known Limitations" section with at least 5 items
  </verify>
  <done>BASELINE_METRICS.md created with comprehensive baseline documentation.</done>
</task>

</tasks>

<verification>
Overall verification steps:
1. BASELINE_METRICS.md exists in `.planning/phases/01-baseline-measurement/`
2. Document contains valid timestamp (not placeholder)
3. All 8 entity types are documented with TP/FN/FP/P/R/F1/F2 for BOTH datasets
4. Overall metrics match JSON output from evaluate_presidio.py
5. Known limitations section has actionable items for future phases including MEAS-02 deferral
6. Document references correct file paths and commands
7. Adversarial Dataset Evaluation section exists with comparison to standard dataset
8. Priority Gaps for Phase 4 section identifies specific edge case failures
</verification>

<success_criteria>
1. BASELINE_METRICS.md created with complete structure
2. Metrics are accurate (match evaluation script output for both datasets)
3. Per-entity breakdown covers all 8 PHI types for both standard and adversarial datasets
4. Known limitations documented for future phases including MEAS-02 deferral to Phase 5
5. Test failure analysis included
6. Document provides defensible baseline for research poster
7. Adversarial vs standard comparison highlights edge case weaknesses
8. Priority gaps documented for Phase 4 pattern improvements
</success_criteria>

<output>
After completion, create `.planning/phases/01-baseline-measurement/01-02-SUMMARY.md`
</output>
