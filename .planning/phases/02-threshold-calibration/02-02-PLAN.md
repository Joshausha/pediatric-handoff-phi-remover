---
phase: 02-threshold-calibration
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - app/config.py
  - app/deidentification.py
  - tests/evaluate_presidio.py
  - .planning/phases/02-threshold-calibration/CALIBRATION_RESULTS.md
autonomous: true

must_haves:
  truths:
    - "config.py contains phi_score_thresholds dict with per-entity thresholds"
    - "deidentification.py uses per-entity thresholds for detection"
    - "validation threshold aligned with detection thresholds (no more 0.35/0.7 mismatch)"
    - "Calibration results documented with before/after metrics comparison"
  artifacts:
    - path: "app/config.py"
      provides: "Per-entity threshold configuration"
      contains: "phi_score_thresholds"
    - path: "app/deidentification.py"
      provides: "Per-entity threshold filtering in detection"
      contains: "phi_score_thresholds"
    - path: ".planning/phases/02-threshold-calibration/CALIBRATION_RESULTS.md"
      provides: "Documented calibration methodology and results"
      min_lines: 50
  key_links:
    - from: "app/deidentification.py"
      to: "app/config.py"
      via: "imports phi_score_thresholds"
      pattern: "settings\\.phi_score_thresholds"
    - from: "tests/evaluate_presidio.py"
      to: "app/config.py"
      via: "uses phi_score_thresholds for evaluation"
      pattern: "phi_score_thresholds"
---

<objective>
Apply calibrated thresholds to codebase and document results.

Purpose: Integrate per-entity thresholds from calibration into production code, fix the validation threshold mismatch (0.35/0.7), and document the complete calibration methodology with before/after comparison.

Output: Updated config.py with per-entity thresholds, updated deidentification.py with aligned thresholds, updated evaluate_presidio.py to use new thresholds, and CALIBRATION_RESULTS.md documenting the process.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-threshold-calibration/02-RESEARCH.md
@.planning/phases/02-threshold-calibration/02-01-SUMMARY.md

# Key files to modify
@app/config.py
@app/deidentification.py
@tests/evaluate_presidio.py
@tests/results/optimal_thresholds.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add per-entity thresholds to config.py</name>
  <files>app/config.py</files>
  <action>
Update config.py to add per-entity threshold configuration:

1. **Add phi_score_thresholds dict:**
   - Read optimal thresholds from tests/results/optimal_thresholds.json
   - Add Field with per-entity threshold values
   - Include inline comments documenting each threshold's rationale
   - Format: `Dict[str, float]` mapping entity_type -> threshold

2. **Mark global threshold as deprecated:**
   - Keep phi_score_threshold (0.35) for backward compatibility
   - Update description to note it's deprecated, use phi_score_thresholds instead

3. **Add calibration metadata fields:**
   - threshold_calibration_date: str (ISO format)
   - threshold_calibration_method: str (methodology description)
   - threshold_calibration_dataset: str (datasets used)

4. **Example structure:**
   ```python
   phi_score_thresholds: Dict[str, float] = Field(
       default={
           "PERSON": 0.35,              # Baseline 98.8% recall, keep aggressive
           "PHONE_NUMBER": 0.40,        # Calibrated from baseline 74.0%
           # ... values from optimal_thresholds.json
       },
       description="Per-entity confidence thresholds (Phase 2 calibrated)"
   )
   ```

5. **Import Dict from typing if not already imported**

Avoid: Removing phi_score_threshold (needed for backward compatibility). Avoid hardcoding thresholds without reading from calibration results.
  </action>
  <verify>
Run: `python -c "from app.config import settings; print(settings.phi_score_thresholds)"`
Expect: Dict with 8 entity types and their thresholds
  </verify>
  <done>
config.py contains phi_score_thresholds dict with per-entity thresholds from calibration, global threshold marked deprecated, calibration metadata included
  </done>
</task>

<task type="auto">
  <name>Task 2: Update deidentification.py to use per-entity thresholds</name>
  <files>app/deidentification.py</files>
  <action>
Modify deidentification.py to use per-entity thresholds:

1. **Update deidentify_text function:**
   - Change analyzer.analyze() to use score_threshold=0.0 (get all detections)
   - Add post-analysis filtering based on per-entity thresholds:
     ```python
     entity_threshold = settings.phi_score_thresholds.get(
         result.entity_type,
         settings.phi_score_threshold  # Fallback to global
     )
     if result.score >= entity_threshold:
         # Keep this detection
     ```
   - Apply this filtering BEFORE deny list filtering (threshold first, then deny list)

2. **Update validate_deidentification function (THRS-02):**
   - Remove hardcoded 0.7 threshold (line ~272)
   - Use same per-entity thresholds as detection for consistency
   - This fixes the 0.35/0.7 mismatch noted in technical debt
   - Update the warning generation to include threshold used:
     ```python
     f"Potential PHI leak: {result.entity_type} "
     f"(score: {result.score:.2f}, threshold: {entity_threshold:.2f})"
     ```

3. **Add helper function (optional but clean):**
   ```python
   def _get_entity_threshold(entity_type: str) -> float:
       """Get confidence threshold for entity type."""
       return settings.phi_score_thresholds.get(
           entity_type,
           settings.phi_score_threshold
       )
   ```

4. **Update logging to show threshold info:**
   - When filtering by threshold, log which threshold was applied
   - Example: `logger.debug(f"Applied threshold {threshold} for {entity_type}")`

Avoid: Breaking existing API (deidentify_text signature unchanged). Avoid removing deny list filtering.
  </action>
  <verify>
Run: `python -c "from app.deidentification import deidentify_text; result = deidentify_text('Call Sarah at 555-1234'); print(result.entity_count)"`
Expect: Returns integer (entities detected with new thresholds)
  </verify>
  <done>
deidentification.py uses per-entity thresholds for detection and validation, no more hardcoded 0.7 validation threshold
  </done>
</task>

<task type="auto">
  <name>Task 3: Document calibration results and run verification</name>
  <files>.planning/phases/02-threshold-calibration/CALIBRATION_RESULTS.md</files>
  <action>
Create comprehensive calibration documentation:

1. **Run before/after comparison:**
   ```bash
   # Run evaluation with new thresholds
   python tests/evaluate_presidio.py --input tests/synthetic_handoffs.json --json > /tmp/after_standard.json
   python tests/evaluate_presidio.py --input tests/adversarial_handoffs.json --json > /tmp/after_adversarial.json
   ```

2. **Create CALIBRATION_RESULTS.md with sections:**

   **Overview:**
   - Calibration date and methodology
   - Datasets used (synthetic seed=42, adversarial seed=43)
   - Decision criteria (F2 optimization, 90% recall floor)

   **Before/After Metrics:**
   | Metric | Before (0.35 global) | After (per-entity) | Change |
   |--------|---------------------|-------------------|--------|
   | Overall Recall | 77.9% | XX.X% | +XX.X |
   | Overall Precision | 87.4% | XX.X% | +/-XX.X |
   | Overall F2 | XX.X% | XX.X% | +XX.X |

   **Per-Entity Thresholds:**
   | Entity | Threshold | Recall | Precision | F2 | Status |
   |--------|-----------|--------|-----------|-----|--------|
   | PERSON | 0.35 | 98.8% | 92% | 97% | >= 90% |
   | ROOM | 0.30 | 68% | 85% | 73% | < 90% (Phase 4) |
   ...

   **Entities Requiring Pattern Work (Phase 4):**
   - List entities that couldn't achieve 90% recall with any threshold
   - Note best achievable recall and recommended Phase 4 priority

   **Threshold Mismatch Fix (THRS-02):**
   - Document: "Detection and validation now use same per-entity thresholds"
   - Before: Detection 0.35, Validation 0.7 (2x inconsistency)
   - After: Both use phi_score_thresholds dict

   **Rationale Summary:**
   - Why per-entity thresholds > global threshold
   - Why F2 > F1 for PHI detection (recall matters more)
   - Conservative approach: when in doubt, lower threshold

3. **Include raw data references:**
   - Link to tests/results/optimal_thresholds.json
   - Link to tests/results/threshold_sweep.json
   - Link to tests/results/pr_curves/

Avoid: Making up metrics - use actual evaluation results. Avoid claiming success for entities that didn't meet 90% recall.
  </action>
  <verify>
Check: CALIBRATION_RESULTS.md exists with before/after comparison
Check: All 8 entity types documented with their thresholds
  </verify>
  <done>
CALIBRATION_RESULTS.md documents complete calibration methodology, before/after metrics, per-entity thresholds, and identifies entities requiring Phase 4 pattern work
  </done>
</task>

</tasks>

<verification>
1. config.py imports work: `python -c "from app.config import settings; print(len(settings.phi_score_thresholds))"`
2. deidentification uses new thresholds: Grep for "phi_score_thresholds" in deidentification.py
3. No hardcoded 0.7 in validation: `grep -n "0.7" app/deidentification.py` returns empty
4. Evaluation runs with new thresholds: `python tests/evaluate_presidio.py -i tests/synthetic_handoffs.json`
5. CALIBRATION_RESULTS.md contains before/after comparison table
</verification>

<success_criteria>
- Per-entity thresholds integrated into production code
- Detection and validation thresholds aligned (THRS-02 complete)
- Recall improvement documented with actual metrics (THRS-04)
- Threshold rationale documented for reproducibility (THRS-03)
- Entities requiring Phase 4 work clearly identified
</success_criteria>

<output>
After completion, create `.planning/phases/02-threshold-calibration/02-02-SUMMARY.md`
</output>
