---
phase: 02-threshold-calibration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/calibrate_thresholds.py
  - tests/results/pr_curves/
  - tests/results/threshold_sweep.json
  - tests/results/optimal_thresholds.json
autonomous: true

must_haves:
  truths:
    - "Precision-recall curves exist for all 8 entity types"
    - "Threshold sweep results show F2 scores at 0.30, 0.40, 0.50, 0.60 for each entity"
    - "Optimal thresholds selected with documented rationale for each entity type"
    - "Both synthetic and adversarial datasets used in calibration"
  artifacts:
    - path: "tests/calibrate_thresholds.py"
      provides: "Threshold calibration script with CLI"
      min_lines: 150
    - path: "tests/results/optimal_thresholds.json"
      provides: "Per-entity optimal thresholds with metrics and rationale"
      contains: "optimal_threshold"
    - path: "tests/results/threshold_sweep.json"
      provides: "Full sweep results for all entity/threshold combinations"
      contains: "f2"
  key_links:
    - from: "tests/calibrate_thresholds.py"
      to: "tests/evaluate_presidio.py"
      via: "imports PresidioEvaluator"
      pattern: "from.*evaluate_presidio import"
    - from: "tests/calibrate_thresholds.py"
      to: "tests/synthetic_handoffs.json"
      via: "loads dataset"
      pattern: "load_dataset"
---

<objective>
Create threshold calibration infrastructure and run per-entity threshold sweep.

Purpose: Generate precision-recall curves and identify optimal thresholds for all 8 entity types using F2 optimization with 90% recall floor. This provides the data-driven foundation for threshold selection.

Output: Calibration script, PR curve visualizations, threshold sweep results JSON, and optimal thresholds JSON with rationale.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-threshold-calibration/02-RESEARCH.md
@.planning/phases/02-threshold-calibration/02-CONTEXT.md

# Key implementation context
@tests/evaluate_presidio.py
@tests/generate_test_data.py
@app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create threshold calibration script</name>
  <files>tests/calibrate_thresholds.py</files>
  <action>
Create a threshold calibration script that:

1. **Core functionality:**
   - Load both synthetic_handoffs.json (seed=42) and adversarial_handoffs.json (seed=43)
   - For each entity type, sweep thresholds [0.30, 0.40, 0.50, 0.60]
   - Calculate precision, recall, F1, F2 at each threshold
   - Apply 90% recall floor: reject thresholds that drop recall below 90%
   - Select optimal threshold: maximize F2 among recall-safe options
   - Tiebreaker: if F2 scores within 0.5%, prefer higher recall

2. **Per-entity threshold implementation:**
   - Modify analyze_text to accept threshold parameter
   - Run Presidio with threshold=0.0 to get all detections with scores
   - Filter results to entity-specific threshold in post-processing
   - Reuse existing PresidioEvaluator logic for TP/FN/FP classification

3. **Output artifacts:**
   - PR curve data for each entity type (precision/recall/thresholds arrays)
   - Full threshold sweep results (all entity × threshold combinations)
   - Optimal thresholds with metrics and rationale string

4. **CLI interface:**
   - `--standard-dataset PATH` (default: tests/synthetic_handoffs.json)
   - `--adversarial-dataset PATH` (default: tests/adversarial_handoffs.json)
   - `--output-dir PATH` (default: tests/results)
   - `--recall-floor FLOAT` (default: 0.90)
   - `--verbose` flag for progress output

5. **Key implementation notes:**
   - Import from evaluate_presidio: PresidioEvaluator class, load_dataset
   - Import from generate_test_data: SyntheticHandoff, PHISpan
   - Use sklearn.metrics.precision_recall_curve for PR curve generation
   - Use matplotlib for PR curve visualization (save as PNG)
   - Calculate F2 = 5 * P * R / (4 * P + R) consistent with evaluate_presidio.py

Avoid: Global threshold optimization (must be per-entity). Avoid hardcoding paths.
  </action>
  <verify>
Run: `python tests/calibrate_thresholds.py --verbose`
Expect: Script completes without error, creates output files in tests/results/
  </verify>
  <done>
calibrate_thresholds.py exists with CLI, runs threshold sweep for 8 entity types × 4 thresholds, outputs results to tests/results/
  </done>
</task>

<task type="auto">
  <name>Task 2: Generate PR curves and optimal thresholds</name>
  <files>
tests/results/pr_curves/PERSON.png
tests/results/pr_curves/PHONE_NUMBER.png
tests/results/pr_curves/EMAIL_ADDRESS.png
tests/results/pr_curves/DATE_TIME.png
tests/results/pr_curves/LOCATION.png
tests/results/pr_curves/MEDICAL_RECORD_NUMBER.png
tests/results/pr_curves/ROOM.png
tests/results/pr_curves/PEDIATRIC_AGE.png
tests/results/threshold_sweep.json
tests/results/optimal_thresholds.json
  </files>
  <action>
Run the calibration script to generate all artifacts:

1. **Execute calibration:**
   ```bash
   python tests/calibrate_thresholds.py --verbose
   ```

2. **Verify output structure:**
   - tests/results/pr_curves/ should contain 8 PNG files (one per entity type)
   - tests/results/threshold_sweep.json should contain all sweep results
   - tests/results/optimal_thresholds.json should contain optimal thresholds

3. **Validate optimal_thresholds.json format:**
   ```json
   {
     "calibration_date": "2026-01-23",
     "datasets": ["synthetic_handoffs.json", "adversarial_handoffs.json"],
     "methodology": "PR curve analysis, F2 optimization, recall>=90% floor",
     "thresholds": {
       "PERSON": {
         "optimal_threshold": 0.35,
         "metrics": {"precision": 0.92, "recall": 0.98, "f2": 0.97},
         "rationale": "Threshold 0.35 maximizes F2=97% while maintaining recall>=90%"
       },
       // ... other entities
     }
   }
   ```

4. **Document any entities that cannot achieve 90% recall:**
   - If an entity's best threshold still has recall < 90%, note in rationale
   - Example: "ROOM: No threshold achieves 90% recall (best: 68% at 0.30). Pattern improvements required (Phase 4)."

5. **Print summary to console:**
   - Show optimal threshold and metrics for each entity
   - Highlight entities that did/didn't meet 90% recall floor
  </action>
  <verify>
Check files exist:
- `ls tests/results/pr_curves/*.png` (should list 8 files)
- `cat tests/results/optimal_thresholds.json` (should show structured JSON)
- `cat tests/results/threshold_sweep.json` (should show sweep data)
  </verify>
  <done>
PR curves generated for all 8 entity types, optimal_thresholds.json contains per-entity thresholds with rationale, threshold_sweep.json contains full sweep data
  </done>
</task>

</tasks>

<verification>
1. calibrate_thresholds.py runs successfully: `python tests/calibrate_thresholds.py --verbose`
2. 8 PR curve PNGs exist in tests/results/pr_curves/
3. optimal_thresholds.json contains thresholds for all 8 entity types
4. Each threshold entry includes metrics (P, R, F2) and rationale string
5. Thresholds respect 90% recall floor (or document inability to achieve it)
</verification>

<success_criteria>
- Calibration script creates reproducible threshold recommendations
- Per-entity approach identifies entity-specific optimal thresholds
- F2 optimization with recall floor produces HIPAA-conservative results
- All artifacts saved for documentation and reproducibility
</success_criteria>

<output>
After completion, create `.planning/phases/02-threshold-calibration/02-01-SUMMARY.md`
</output>
