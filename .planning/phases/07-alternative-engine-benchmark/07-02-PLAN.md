---
phase: 07-alternative-engine-benchmark
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/benchmark_bert.py
  - configs/bert_entity_mapping.json
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "Stanford BERT model is downloaded and loadable"
    - "TransformersNlpEngine is configured with Stanford model"
    - "BERT entity types are mapped to Presidio entity types"
    - "BERT-based analyzer can process handoff text and return PHI spans"
  artifacts:
    - path: "scripts/benchmark_bert.py"
      provides: "Stanford BERT benchmarking script"
      min_lines: 120
    - path: "configs/bert_entity_mapping.json"
      provides: "Stanford BERT to Presidio entity mapping"
      contains: "PATIENT"
  key_links:
    - from: "scripts/benchmark_bert.py"
      to: "StanfordAIMI/stanford-deidentifier-base"
      via: "HuggingFace model loading"
      pattern: "stanford-deidentifier-base"
    - from: "scripts/benchmark_bert.py"
      to: "tests/synthetic_handoffs.json"
      via: "dataset loading"
      pattern: 'data\["handoffs"\]'
---

<objective>
Integrate Stanford BERT (StanfordAIMI/stanford-deidentifier-base) as a Presidio NER backend using TransformersNlpEngine.

Purpose: Enable comparison of transformer-based de-identification against current spaCy NER + custom patterns approach.

Output: Working Stanford BERT integration with entity mapping and benchmarking script.
</objective>

<execution_context>
@/Users/joshpankin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/joshpankin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-alternative-engine-benchmark/07-RESEARCH.md
@.planning/phases/07-alternative-engine-benchmark/07-CONTEXT.md
@tests/evaluate_presidio.py
@app/config.py
@app/recognizers/pediatric.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install transformer dependencies and download Stanford BERT model</name>
  <files>requirements.txt</files>
  <action>
1. Add transformer dependencies to requirements.txt:
   - transformers>=4.0.0
   - torch (CPU version is fine: torch --index-url https://download.pytorch.org/whl/cpu)

2. Install dependencies:
   ```bash
   pip install transformers torch --index-url https://download.pytorch.org/whl/cpu
   ```

3. Download Stanford BERT model (this triggers HuggingFace caching):
   ```python
   from transformers import AutoTokenizer, AutoModelForTokenClassification
   tokenizer = AutoTokenizer.from_pretrained("StanfordAIMI/stanford-deidentifier-base")
   model = AutoModelForTokenClassification.from_pretrained("StanfordAIMI/stanford-deidentifier-base")
   print(f"Model labels: {model.config.id2label}")
   ```

4. Document all entity labels output by the model (needed for entity mapping in Task 2).

IMPORTANT: Model is ~440MB. First download will take time. Subsequent runs use cache.
  </action>
  <verify>
```bash
python -c "from transformers import AutoModelForTokenClassification; m = AutoModelForTokenClassification.from_pretrained('StanfordAIMI/stanford-deidentifier-base'); print(list(m.config.id2label.values()))"
```
Outputs list of entity labels from Stanford BERT.
  </verify>
  <done>Transformer dependencies installed, Stanford BERT model cached locally, entity labels documented.</done>
</task>

<task type="auto">
  <name>Task 2: Create BERT entity mapping and benchmark script</name>
  <files>configs/bert_entity_mapping.json, scripts/benchmark_bert.py</files>
  <action>
1. **Create directories first:**
   ```bash
   mkdir -p configs scripts
   ```

2. Create `configs/bert_entity_mapping.json` mapping Stanford labels to Presidio entities:
   ```json
   {
     "PATIENT": "PERSON",
     "STAFF": "PERSON",
     "AGE": "PEDIATRIC_AGE",
     "DATE": "DATE_TIME",
     "PHONE": "PHONE_NUMBER",
     "HOSP": "LOCATION",
     "HOSPITAL": "LOCATION",
     "IDNUM": "MEDICAL_RECORD_NUMBER",
     "O": null
   }
   ```
   Update mapping based on actual labels discovered in Task 1.

3. Create `scripts/benchmark_bert.py`:
   - **Import EvaluationMetrics and DetectionResult:**
     ```python
     import sys
     from pathlib import Path
     sys.path.insert(0, str(Path(__file__).parent.parent))
     from tests.evaluate_presidio import EvaluationMetrics, DetectionResult
     ```

   - **Import and wire custom pediatric recognizers:**
     ```python
     from app.recognizers.pediatric import get_pediatric_recognizers

     # After creating analyzer:
     pediatric_recognizers = get_pediatric_recognizers()
     for recognizer in pediatric_recognizers:
         analyzer.registry.add_recognizer(recognizer)
     ```

   - Configure TransformersNlpEngine with Stanford BERT model:
     ```python
     nlp_config = {
         "nlp_engine_name": "transformers",
         "models": [{
             "lang_code": "en",
             "model_name": {
                 "spacy": "en_core_web_sm",
                 "transformers": "StanfordAIMI/stanford-deidentifier-base"
             }
         }]
     }
     ```

   - Create AnalyzerEngine with TransformersNlpEngine

   - **Load dataset correctly (dict with "handoffs" key):**
     ```python
     with open(input_path) as f:
         data = json.load(f)
     handoffs = data["handoffs"]  # Dataset is dict with "handoffs" key
     # 500 standard handoffs in synthetic_handoffs.json
     # 100 adversarial handoffs in adversarial_handoffs.json
     ```

   - Map BERT entities to Presidio types using configs/bert_entity_mapping.json
   - Compare against ground truth, calculate metrics
   - Include --weighted flag for spoken handoff weights
   - **REQUIRED: Include --sample N flag to run on subset first (CPU inference is slow)**
     ```python
     parser.add_argument('--sample', type=int, default=0,
                         help='Run on first N samples only (0=all). Use for initial testing.')
     ```

4. **Test on 20-sample subset first with --sample flag:**
   ```bash
   python scripts/benchmark_bert.py --sample 20 --verbose
   ```
   Estimate full runtime: If 20 samples take X minutes, 600 samples will take ~30X minutes.

RUNTIME WARNING: BERT inference on CPU takes 5-10 seconds per handoff.
- 20 samples: ~2-3 minutes
- 100 samples: ~10-15 minutes
- 600 samples (full): ~1-2 hours

Plan 07-03 will use --sample for initial validation before committing to full run.
  </action>
  <verify>
```bash
# Run BERT benchmark on small sample - MUST use --sample flag
python scripts/benchmark_bert.py --sample 20 --verbose
```
Script runs without errors and produces metrics output within ~3 minutes.
  </verify>
  <done>BERT entity mapping complete, custom pediatric recognizers wired, benchmark script produces precision/recall/F2 metrics, --sample flag works for subset testing.</done>
</task>

</tasks>

<verification>
1. `python -c "from transformers import AutoModelForTokenClassification; print('transformers OK')"` succeeds
2. Stanford BERT model cached: `ls ~/.cache/huggingface/hub/models--StanfordAIMI--*` shows directory
3. Directories exist: `ls configs/ scripts/`
4. `python scripts/benchmark_bert.py --sample 10` runs and outputs metrics within ~1 minute
5. Entity mapping documented in `configs/bert_entity_mapping.json`
6. Script loads dataset as `data["handoffs"]`, not as raw list
7. Custom pediatric recognizers are imported from app/recognizers/pediatric.py and registered
</verification>

<success_criteria>
- Stanford BERT model downloaded and loadable via TransformersNlpEngine
- Entity mapping covers all BERT output labels
- Custom pediatric recognizers imported and wired with registry.add_recognizer()
- Benchmark script runs on sample dataset and produces comparable metrics
- --sample flag works for subset testing (critical for 07-03 runtime management)
- CPU inference time documented (needed for planning full benchmark in 07-03)
- Dataset accessed via data["handoffs"] key
- Ready for comparative benchmark in 07-03
</success_criteria>

<output>
After completion, create `.planning/phases/07-alternative-engine-benchmark/07-02-SUMMARY.md`
</output>
