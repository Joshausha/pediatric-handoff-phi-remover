---
phase: 07-alternative-engine-benchmark
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/benchmark_bert.py
  - configs/bert_entity_mapping.json
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "Stanford BERT model is downloaded and loadable"
    - "TransformersNlpEngine is configured with Stanford model"
    - "BERT entity types are mapped to Presidio entity types"
    - "BERT-based analyzer can process handoff text and return PHI spans"
  artifacts:
    - path: "scripts/benchmark_bert.py"
      provides: "Stanford BERT benchmarking script"
      min_lines: 120
    - path: "configs/bert_entity_mapping.json"
      provides: "Stanford BERT to Presidio entity mapping"
      contains: "PATIENT"
  key_links:
    - from: "scripts/benchmark_bert.py"
      to: "StanfordAIMI/stanford-deidentifier-base"
      via: "HuggingFace model loading"
      pattern: "stanford-deidentifier-base"
    - from: "scripts/benchmark_bert.py"
      to: "tests/synthetic_handoffs.json"
      via: "dataset loading"
      pattern: "synthetic_handoffs.json"
---

<objective>
Integrate Stanford BERT (StanfordAIMI/stanford-deidentifier-base) as a Presidio NER backend using TransformersNlpEngine.

Purpose: Enable comparison of transformer-based de-identification against current spaCy NER + custom patterns approach.

Output: Working Stanford BERT integration with entity mapping and benchmarking script.
</objective>

<execution_context>
@/Users/joshpankin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/joshpankin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-alternative-engine-benchmark/07-RESEARCH.md
@.planning/phases/07-alternative-engine-benchmark/07-CONTEXT.md
@tests/evaluate_presidio.py
@app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install transformer dependencies and download Stanford BERT model</name>
  <files>requirements.txt</files>
  <action>
1. Add transformer dependencies to requirements.txt:
   - transformers>=4.0.0
   - torch (CPU version is fine: torch --index-url https://download.pytorch.org/whl/cpu)

2. Install dependencies:
   ```bash
   pip install transformers torch --index-url https://download.pytorch.org/whl/cpu
   ```

3. Download Stanford BERT model (this triggers HuggingFace caching):
   ```python
   from transformers import AutoTokenizer, AutoModelForTokenClassification
   tokenizer = AutoTokenizer.from_pretrained("StanfordAIMI/stanford-deidentifier-base")
   model = AutoModelForTokenClassification.from_pretrained("StanfordAIMI/stanford-deidentifier-base")
   print(f"Model labels: {model.config.id2label}")
   ```

4. Document all entity labels output by the model (needed for entity mapping in Task 2).

IMPORTANT: Model is ~440MB. First download will take time. Subsequent runs use cache.
  </action>
  <verify>
```bash
python -c "from transformers import AutoModelForTokenClassification; m = AutoModelForTokenClassification.from_pretrained('StanfordAIMI/stanford-deidentifier-base'); print(list(m.config.id2label.values()))"
```
Outputs list of entity labels from Stanford BERT.
  </verify>
  <done>Transformer dependencies installed, Stanford BERT model cached locally, entity labels documented.</done>
</task>

<task type="auto">
  <name>Task 2: Create BERT entity mapping and benchmark script</name>
  <files>configs/bert_entity_mapping.json, scripts/benchmark_bert.py</files>
  <action>
1. Create `configs/bert_entity_mapping.json` mapping Stanford labels to Presidio entities:
   ```json
   {
     "PATIENT": "PERSON",
     "STAFF": "PERSON",
     "AGE": "PEDIATRIC_AGE",
     "DATE": "DATE_TIME",
     "PHONE": "PHONE_NUMBER",
     "HOSP": "LOCATION",
     "HOSPITAL": "LOCATION",
     "IDNUM": "MEDICAL_RECORD_NUMBER",
     "O": null
   }
   ```
   Update mapping based on actual labels discovered in Task 1.

2. Create `scripts/benchmark_bert.py`:
   - Configure TransformersNlpEngine with Stanford BERT model:
     ```python
     nlp_config = {
         "nlp_engine_name": "transformers",
         "models": [{
             "lang_code": "en",
             "model_name": {
                 "spacy": "en_core_web_sm",
                 "transformers": "StanfordAIMI/stanford-deidentifier-base"
             }
         }]
     }
     ```
   - Create AnalyzerEngine with TransformersNlpEngine
   - Add custom pediatric recognizers from app/recognizers/ (same patterns as Presidio baseline)
   - Load synthetic_handoffs.json and adversarial_handoffs.json
   - Run BERT-based analyzer on each handoff
   - Map BERT entities to Presidio types using configs/bert_entity_mapping.json
   - Compare against ground truth, calculate metrics
   - Include --weighted flag for spoken handoff weights
   - Include --sample N flag to run on subset first (CPU inference is slow)

3. Test on 50-sample subset first:
   ```bash
   python scripts/benchmark_bert.py --sample 50 --verbose
   ```
   Estimate full runtime from subset timing.

NOTE: BERT inference on CPU may take 5-10 seconds per handoff. Plan for 1-2 hours for full 600-handoff benchmark.
  </action>
  <verify>
```bash
# Run BERT benchmark on small sample
python scripts/benchmark_bert.py --sample 20 --verbose
```
Script runs without errors and produces metrics output within reasonable time (~2-3 minutes for 20 samples).
  </verify>
  <done>BERT entity mapping complete, benchmark script produces precision/recall/F2 metrics, CPU inference time documented.</done>
</task>

</tasks>

<verification>
1. `python -c "from transformers import AutoModelForTokenClassification; print('transformers OK')"` succeeds
2. Stanford BERT model cached: `ls ~/.cache/huggingface/hub/models--StanfordAIMI--*` shows directory
3. `python scripts/benchmark_bert.py --sample 10` runs and outputs metrics
4. Entity mapping documented in `configs/bert_entity_mapping.json`
</verification>

<success_criteria>
- Stanford BERT model downloaded and loadable via TransformersNlpEngine
- Entity mapping covers all BERT output labels
- Benchmark script runs on sample dataset and produces comparable metrics
- CPU inference time documented (needed for planning full benchmark in 07-03)
- Ready for comparative benchmark in 07-03
</success_criteria>

<output>
After completion, create `.planning/phases/07-alternative-engine-benchmark/07-02-SUMMARY.md`
</output>
