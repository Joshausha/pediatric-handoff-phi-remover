---
phase: 07-alternative-engine-benchmark
plan: 03
type: execute
wave: 2
depends_on: ["07-01", "07-02"]
files_modified:
  - docs/ENGINE_BENCHMARK.md
  - .planning/phases/07-alternative-engine-benchmark/07-03-SUMMARY.md
autonomous: false

must_haves:
  truths:
    - "All three engines benchmarked on same 600-handoff dataset"
    - "Weighted recall/precision/F2 calculated for each engine"
    - "Engine decision documented with supporting evidence"
    - "Residual risk documented for selected approach"
  artifacts:
    - path: "docs/ENGINE_BENCHMARK.md"
      provides: "Comparative benchmark results and decision"
      min_lines: 100
      contains: "Decision"
  key_links:
    - from: "docs/ENGINE_BENCHMARK.md"
      to: "tests/synthetic_handoffs.json"
      via: "benchmark dataset reference"
      pattern: "600 handoffs"
    - from: "docs/ENGINE_BENCHMARK.md"
      to: "scripts/benchmark_philter.py"
      via: "Philter benchmark results"
      pattern: "Philter"
    - from: "docs/ENGINE_BENCHMARK.md"
      to: "scripts/benchmark_bert.py"
      via: "BERT benchmark results"
      pattern: "Stanford BERT"
---

<objective>
Run comparative benchmark of all three engines (Presidio, Philter-UCSF, Stanford BERT) on the full synthetic dataset and document the engine selection decision.

Purpose: Determine whether switching engines provides >5% weighted recall improvement over current Presidio setup to justify the migration effort.

Output: ENGINE_BENCHMARK.md with comparative results, decision, and residual risk documentation.
</objective>

<execution_context>
@/Users/joshpankin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/joshpankin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-alternative-engine-benchmark/07-RESEARCH.md
@.planning/phases/07-alternative-engine-benchmark/07-CONTEXT.md
@.planning/phases/07-alternative-engine-benchmark/07-01-SUMMARY.md
@.planning/phases/07-alternative-engine-benchmark/07-02-SUMMARY.md
@tests/evaluate_presidio.py
@scripts/benchmark_philter.py
@scripts/benchmark_bert.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Spot-check synthetic data annotation quality</name>
  <files>None (analysis only)</files>
  <action>
1. Load 20 random samples from synthetic_handoffs.json:
   ```python
   import json
   import random
   with open('tests/synthetic_handoffs.json') as f:
       data = json.load(f)
   samples = random.sample(data["handoffs"], 20)  # Note: data["handoffs"], not data
   ```

2. For each sample, manually review:
   - Are all PHI spans correctly annotated?
   - Are there any missed PHI in the text?
   - Are entity types correctly labeled?
3. Document any annotation issues found
4. If issues found:
   - Fix annotations in source dataset
   - Ensure same fixes apply to all engine benchmarks (fair comparison)
5. Document confidence level in annotation quality
  </action>
  <verify>
Print 5 sample texts with their annotations visually highlighted.
Document annotation quality assessment (pass/fix needed).
  </verify>
  <done>20 samples spot-checked, annotation quality documented, any fixes applied to source data.</done>
</task>

<task type="auto">
  <name>Task 2: Run full benchmark on all three engines</name>
  <files>None (benchmark execution)</files>
  <action>
**Dataset structure reminder:** Both JSON files are dicts with "metadata" and "handoffs" keys.
- synthetic_handoffs.json: 500 handoffs in data["handoffs"]
- adversarial_handoffs.json: 100 handoffs in data["handoffs"]
- Total: 600 handoffs

1. **Create results directory:**
   ```bash
   mkdir -p results
   ```

2. Run Presidio baseline (current system):
   ```bash
   python tests/evaluate_presidio.py --input tests/synthetic_handoffs.json --weighted -o results/presidio_standard.txt
   python tests/evaluate_presidio.py --input tests/adversarial_handoffs.json --weighted -o results/presidio_adversarial.txt
   ```

3. Run Philter-UCSF:
   ```bash
   python scripts/benchmark_philter.py --input tests/synthetic_handoffs.json --weighted -o results/philter_standard.txt
   python scripts/benchmark_philter.py --input tests/adversarial_handoffs.json --weighted -o results/philter_adversarial.txt
   ```

4. **Run Stanford BERT (with progress logging for long runtime):**

   **IMPORTANT: BERT on CPU takes 5-10 seconds per handoff = 1-2 hours for 600 handoffs.**

   Strategy for handling long runtime:
   a. First, run --sample 50 to validate script works and estimate time:
      ```bash
      time python scripts/benchmark_bert.py --sample 50 --weighted --verbose
      ```
   b. Calculate estimated full runtime: (elapsed_time / 50) * 600
   c. If estimated time is acceptable (<2 hours), proceed with full run:
      ```bash
      python scripts/benchmark_bert.py --input tests/synthetic_handoffs.json --weighted -o results/bert_standard.txt 2>&1 | tee bert_progress.log &
      ```
   d. Monitor progress with: `tail -f bert_progress.log`
   e. Run adversarial set:
      ```bash
      python scripts/benchmark_bert.py --input tests/adversarial_handoffs.json --weighted -o results/bert_adversarial.txt
      ```

5. Collect metrics from each run:
   - Weighted recall (primary decision metric)
   - Weighted precision (must be >70%)
   - Weighted F2
   - Per-entity breakdown for high-weight types (GUARDIAN_NAME, ROOM, PERSON)

6. Calculate improvement thresholds:
   - Philter improvement = Philter_weighted_recall - Presidio_weighted_recall
   - BERT improvement = BERT_weighted_recall - Presidio_weighted_recall
   - Flag engines with >5% improvement as candidates
  </action>
  <verify>
All 6 result files exist in results/ directory:
```bash
ls -la results/presidio_*.txt results/philter_*.txt results/bert_*.txt
```
Each result file contains weighted metrics section.
  </verify>
  <done>All three engines benchmarked on full dataset (500 standard + 100 adversarial = 600 total), metrics collected.</done>
</task>

<task type="checkpoint:decision" gate="blocking">
  <decision>Select de-identification engine based on benchmark results</decision>
  <context>
Decision criteria from CONTEXT.md:
- Primary metric: Weighted recall (spoken handoff relevance)
- Precision floor: >70% required
- Improvement threshold: >5% weighted recall improvement justifies switch
- If no engine beats Presidio by 5%+: Consider hybrid approach

Present benchmark results and recommendation.
  </context>
  <options>
    <option id="option-presidio">
      <name>Stick with Presidio + spaCy + custom patterns</name>
      <pros>Already integrated, custom patterns tuned, no migration needed</pros>
      <cons>May leave performance on table if alternatives are better</cons>
      <when>No engine achieves >5% weighted recall improvement</when>
    </option>
    <option id="option-philter">
      <name>Switch to Philter-UCSF</name>
      <pros>State-of-the-art rule-based, proven 99.5% on written notes</pros>
      <cons>Pattern translation effort, new dependency, may not improve spoken handoff performance</cons>
      <when>Philter weighted recall > Presidio + 5% AND precision >70%</when>
    </option>
    <option id="option-bert">
      <name>Switch to Stanford BERT (as Presidio backend)</name>
      <pros>Transformer-based NER may catch more names, works with existing custom patterns</pros>
      <cons>Slow CPU inference, larger memory footprint, may perform worse on spoken text</cons>
      <when>BERT weighted recall > Presidio + 5% AND precision >70%</when>
    </option>
    <option id="option-hybrid">
      <name>Hybrid approach (BERT for PERSON, Presidio for custom patterns)</name>
      <pros>Best of both worlds - transformer NER + pediatric patterns</pros>
      <cons>More complex setup, two engines to maintain</cons>
      <when>No single engine wins by 5%, but BERT excels at PERSON detection</when>
    </option>
  </options>
  <resume-signal>Select: option-presidio, option-philter, option-bert, or option-hybrid</resume-signal>
</task>

<task type="auto">
  <name>Task 3: Document benchmark results and decision</name>
  <files>docs/ENGINE_BENCHMARK.md</files>
  <action>
Create comprehensive benchmark documentation:

1. Executive Summary
   - Decision made and rationale
   - Key metrics comparison table
   - Residual risk for selected approach

2. Methodology
   - Dataset: 600 handoffs (500 standard + 100 adversarial)
   - Note: Dataset loaded via `data["handoffs"]` from JSON dict structure
   - Metrics: Weighted recall (primary), weighted precision, weighted F2
   - Weights: PERSON/GUARDIAN=5, ROOM=4, PHONE/DATE=2, MRN=1, others=0
   - Improvement threshold: >5% weighted recall

3. Results Table
   | Engine | Weighted Recall | Weighted Precision | Weighted F2 | vs Presidio |
   |--------|-----------------|--------------------| ------------|-------------|
   | Presidio | X.X% | X.X% | X.X% | baseline |
   | Philter | X.X% | X.X% | X.X% | +/-X.X% |
   | BERT | X.X% | X.X% | X.X% | +/-X.X% |

4. Per-Entity Breakdown (high-weight entities)
   - PERSON/GUARDIAN_NAME: Which engine performed best?
   - ROOM: Pattern coverage comparison
   - Any entity type where one engine significantly outperforms?

5. Decision Analysis
   - Did any engine meet the >5% improvement threshold?
   - Precision floor check (all >70%?)
   - Hybrid evaluation if no clear winner

6. Residual Risk
   - Document remaining gaps with selected approach
   - Mitigation strategies (user review, additional patterns)

7. Next Steps
   - If switching: Migration plan outline
   - If staying with Presidio: Resume 05-04 expert review
  </action>
  <verify>
docs/ENGINE_BENCHMARK.md exists with all required sections.
Decision is clearly stated with supporting metrics.
  </verify>
  <done>ENGINE_BENCHMARK.md complete with comparative results, decision, and residual risk documentation.</done>
</task>

</tasks>

<verification>
1. All benchmark result files exist in results/ directory
2. docs/ENGINE_BENCHMARK.md contains:
   - Clear decision statement
   - Comparative metrics table
   - Per-entity analysis for high-weight types
   - Residual risk documentation
3. Decision aligns with stated criteria (>5% improvement, >70% precision)
</verification>

<success_criteria>
- All three engines benchmarked on same 600-handoff dataset (500 standard + 100 adversarial)
- Dataset accessed correctly via data["handoffs"] key
- Weighted metrics calculated and compared
- Decision documented with clear rationale
- Residual risk documented for selected approach
- Next steps defined (migration or resume Phase 5)
</success_criteria>

<output>
After completion, create `.planning/phases/07-alternative-engine-benchmark/07-03-SUMMARY.md`
</output>
