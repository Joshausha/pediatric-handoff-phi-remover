---
phase: 14-report-refinement
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/evaluate_presidio.py
autonomous: true

must_haves:
  truths:
    - "Running --weighted shows all three metrics in a single summary table"
    - "Unweighted recall is labeled 'Safety Floor' inline"
    - "Two weight tables are displayed side-by-side (frequency-sorted and risk-sorted)"
    - "Entities with weight divergence >2.0 are visually marked"
    - "Concrete example explains why frequency and risk metrics differ"
    - "Zero-weight entities (EMAIL_ADDRESS, PEDIATRIC_AGE) have explanatory note"
  artifacts:
    - path: "tests/evaluate_presidio.py"
      provides: "Polished three-metric report generation"
      contains: "METRIC SUMMARY"
  key_links:
    - from: "tests/evaluate_presidio.py"
      to: "app/config.py"
      via: "settings.spoken_handoff_weights, settings.spoken_handoff_risk_weights"
      pattern: "freq_weights|risk_weights"
---

<objective>
Refine the evaluation report to display three-metric summary with clear divergence explanation.

Purpose: Users need to understand what each metric measures (frequency vs risk) and why they might differ. The current report shows the data but lacks clear formatting and explanation.

Output: Polished `generate_report` function that produces clear, well-formatted three-metric reports with weight comparison tables and divergence documentation.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-report-refinement/14-CONTEXT.md
@.planning/phases/14-report-refinement/14-RESEARCH.md
@tests/evaluate_presidio.py
@app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Refactor three-metric summary table</name>
  <files>tests/evaluate_presidio.py</files>
  <action>
Modify the `generate_report` method (around lines 484-506) to display all three metric types in a unified summary table:

1. Replace the scattered metric displays with a single formatted table:
```
EVALUATION METRICS SUMMARY:

                            Recall  Precision    F2
  --------------------------------------------------
  Unweighted (Safety Floor)  86.4%     69.0%  81.2%
  Frequency-weighted         94.2%     73.5%  88.9%
  Risk-weighted              91.8%     70.2%  86.1%
```

2. Use f-string formatting with `.1%` precision for all percentages
3. Use fixed-width columns with left-alignment for metric names (`{name:<25}`)
4. Keep the "Safety Floor" annotation inline with Unweighted row
5. Remove the separate FREQUENCY-WEIGHTED and RISK-WEIGHTED sections (consolidate into summary table)
6. Keep the existing METRIC SUMMARY section header but make it the primary display

Do NOT change:
- The unweighted OVERALL METRICS section (lines 452-468) - keep as-is for backward compatibility
- The confidence intervals section (lines 471-482)
- Per-type performance section (lines 542-551)
- Failure details section (lines 554-565)
  </action>
  <verify>
Run `python tests/evaluate_presidio.py --weighted -i tests/synthetic_handoffs.json 2>&1 | head -50` and confirm:
1. Three-metric summary table appears with all three rows
2. Columns are aligned (Recall, Precision, F2)
3. "Safety Floor" appears inline with Unweighted row
4. All percentages use .1% format (e.g., 86.4% not 86.38%)
  </verify>
  <done>
Three-metric summary table displays in unified format with aligned columns, inline Safety Floor annotation, and consistent .1% precision.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add side-by-side weight tables and divergence explanation</name>
  <files>tests/evaluate_presidio.py</files>
  <action>
After the three-metric summary table, add:

1. **Side-by-side weight comparison tables** (replace the existing single table at lines 508-515):
```
WEIGHT COMPARISON:

  Frequency (How Often Spoken)      Risk (Severity If Leaked)
  --------------------------        -------------------------
  PERSON               5.0          MEDICAL_RECORD_NUMBER 5.0
* GUARDIAN_NAME        4.0        * PERSON                5.0
* ROOM                 4.0          PHONE_NUMBER          4.0
  ...                               ...

* = Weight divergence >2.0 between frequency and risk
```

Implementation:
- Sort entities by frequency weight for left table
- Sort entities by risk weight for right table
- Mark entities with asterisk (*) if abs(freq_weight - risk_weight) > 2.0
- Use fixed-width columns: `{entity:<24} {weight:>4.1f}`

2. **Zero-weight entity note** after the weight tables:
```
NOTE: Zero-weight entities
  EMAIL_ADDRESS (0.0): Never spoken in verbal handoffs
  PEDIATRIC_AGE (0.0): Not PHI under HIPAA unless age 90+
  These entities count in unweighted metrics but are excluded from weighted calculations.
```

3. **Concrete divergence explanation** (add new section):
```
METRIC DIVERGENCE EXPLANATION:

  Frequency-weighted recall: 94.2%
  Risk-weighted recall:      91.8%
  Gap: +2.4 percentage points

  Why they differ:
  - MEDICAL_RECORD_NUMBER has low frequency weight (0.5) but high risk weight (5.0)
  - When MRN detection underperforms, frequency recall stays high
    (dominated by PERSON with weight 5.0)
  - But risk-weighted recall drops significantly
    (MRN and PERSON have equal weight, MRN drags down average)

  Guidance:
  - Unweighted recall is your HIPAA compliance floor (all entities equal)
  - Frequency-weighted reflects operational reality (what's actually spoken)
  - Risk-weighted highlights critical vulnerabilities (severity if leaked)
```

Implementation:
- Calculate actual gap: `freq_recall - risk_recall`
- Use `{gap:+.1f}` format to show + or - sign
- Keep the explanation text exactly as shown (hardcoded is fine - it explains the design, not dynamic data)

Do NOT:
- Remove any existing sections
- Change the overall report structure
- Add external dependencies
  </action>
  <verify>
Run `python tests/evaluate_presidio.py --weighted -i tests/synthetic_handoffs.json 2>&1 | grep -A 30 "WEIGHT COMPARISON"` and confirm:
1. Two columns of weights appear (frequency and risk sorted)
2. Asterisks mark divergent entities (MRN should have one)
3. Zero-weight entity note appears
4. Divergence explanation section shows actual gap with +/- sign
  </verify>
  <done>
Report includes side-by-side weight tables with divergence markers, zero-weight entity annotation, and concrete explanation of why frequency and risk metrics differ.
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

1. Run full evaluation with --weighted flag:
   ```bash
   python tests/evaluate_presidio.py --weighted -i tests/synthetic_handoffs.json
   ```

2. Verify report contains all REPT requirements:
   - [ ] REPT-01: Unweighted metrics displayed (in summary table)
   - [ ] REPT-02: Frequency-weighted metrics displayed (in summary table)
   - [ ] REPT-03: Risk-weighted metrics displayed (in summary table)
   - [ ] REPT-04: "Safety Floor" label on unweighted row
   - [ ] REPT-05: Side-by-side weight comparison tables
   - [ ] REPT-06: Divergence explanation section with concrete example

3. Run existing tests to ensure no regressions:
   ```bash
   pytest tests/test_weighted_metrics.py -v
   ```

4. Confirm CI passes:
   ```bash
   pytest tests/ -v --tb=short
   ```
</verification>

<success_criteria>
- All six REPT requirements verified in report output
- Three-metric summary table shows Recall, Precision, F2 in aligned columns
- Side-by-side weight tables highlight entities with >2.0 divergence
- Zero-weight entities have explanatory note
- Divergence explanation includes quantified gap and guidance
- All existing tests pass (no regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/14-report-refinement/14-01-SUMMARY.md`
</output>
