---
phase: 16-integration-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/integration/__init__.py
  - tests/integration/test_full_evaluation.py
  - tests/integration/test_regression.py
  - tests/baselines/regression.json
  - .github/workflows/test.yml
autonomous: true

must_haves:
  truths:
    - "pytest tests/integration/ passes with all three metric types calculated"
    - "Unweighted recall >=85% threshold enforced as CI failure condition"
    - "CI workflow runs smoke tests on PRs and full validation on main branch"
    - "Regression baseline detects metric changes between runs"
  artifacts:
    - path: "tests/integration/test_full_evaluation.py"
      provides: "End-to-end validation orchestration"
      min_lines: 50
    - path: "tests/integration/test_regression.py"
      provides: "Regression baseline assertions"
      min_lines: 30
    - path: "tests/baselines/regression.json"
      provides: "Committed baseline metrics for regression detection"
      contains: "freq_weighted_recall"
    - path: ".github/workflows/test.yml"
      provides: "Tiered CI with smoke test and full validation"
      contains: "integration"
  key_links:
    - from: "tests/integration/test_full_evaluation.py"
      to: "tests/run_validation.py"
      via: "import run_validation"
      pattern: "from tests.run_validation import"
    - from: "tests/integration/test_regression.py"
      to: "tests/baselines/regression.json"
      via: "data_regression fixture loads baseline"
      pattern: "data_regression"
---

<objective>
Create integration test infrastructure for end-to-end validation of the dual-weight recall framework.

Purpose: Ensure the complete evaluation pipeline (run_validation.py -> weighted metrics -> report) works correctly and catches regressions before they reach production. This closes the validation loop for v2.2.

Output: Integration test suite with regression baselines, tiered CI workflow (smoke test on PRs, full validation on main).
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-integration-validation/16-CONTEXT.md
@.planning/phases/16-integration-validation/16-RESEARCH.md

# Existing infrastructure to wrap
@tests/run_validation.py
@tests/evaluate_presidio.py
@tests/test_weighted_metrics.py
@app/config.py
@.github/workflows/test.yml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create integration test infrastructure</name>
  <files>
    tests/integration/__init__.py
    tests/integration/test_full_evaluation.py
    tests/integration/test_regression.py
  </files>
  <action>
Create the integration test directory and files:

1. Create `tests/integration/__init__.py` (empty file for package)

2. Create `tests/integration/test_full_evaluation.py`:
   - Import `run_validation` from `tests.run_validation`
   - Test `test_synthetic_dataset_evaluation()`:
     - Run validation on `tests/synthetic_handoffs.json` with `n_bootstrap=1000` (reduced for speed)
     - Assert `results["metrics"]["recall"] >= 0.85` (unweighted recall floor - HARD FAIL)
     - Assert all three metric types present: `recall`, `freq_weighted_recall`, `risk_weighted_recall` (from entity_stats)
     - Assert all metrics are float type
   - Test `test_three_metrics_calculated()`:
     - Verify frequency-weighted and risk-weighted metrics differ (frequency > risk recall expected pattern)
     - Document divergence observation in test docstring (not hard assertion per CONTEXT.md)
   - Test `test_config_weights_loaded()`:
     - Import settings from `app.config`
     - Assert `spoken_handoff_weights` exists and has float values
     - Assert `spoken_handoff_risk_weights` exists and has float values
     - Assert both dicts cover same entity types (CONF-01, CONF-02, CONF-03 validation)

3. Create `tests/integration/test_regression.py`:
   - Test `test_unweighted_recall_floor()`:
     - Run validation and assert `recall >= 0.85` (this is the HIPAA safety floor)
     - Clear error message on failure: "REGRESSION: Unweighted recall {value:.1%} below 85% floor"
   - Test `test_metrics_match_baseline()`:
     - Load baseline from `tests/baselines/regression.json`
     - Compare current metrics against baseline (with tolerance)
     - Use `pytest.approx()` for float comparisons with `rel=0.01` (1% tolerance)
     - Metrics to compare: recall, precision, f2, freq_weighted_recall, risk_weighted_recall

Use pytest fixtures for shared validation run (run once, use in multiple tests).
Do NOT use pytest-regressions library - use simple JSON file comparison instead (fewer dependencies).
  </action>
  <verify>
Run: `pytest tests/integration/ -v --tb=short`
Expected: All tests pass (may need initial baseline creation)
  </verify>
  <done>
Integration tests exist and pass. Tests validate:
- Three metric types calculated correctly
- Unweighted recall floor enforced (>=85%)
- Config weights properly loaded from pydantic settings
- Regression detection against committed baseline
  </done>
</task>

<task type="auto">
  <name>Task 2: Create regression baseline and CI workflow</name>
  <files>
    tests/baselines/regression.json
    .github/workflows/test.yml
  </files>
  <action>
1. Create `tests/baselines/regression.json`:
   - Run the validation once to get current metrics: `python -c "from tests.run_validation import run_validation; from pathlib import Path; import json; r = run_validation(Path('tests/synthetic_handoffs.json'), n_bootstrap=1000, verbose=False); print(json.dumps({k: round(v, 4) for k, v in r['metrics'].items() if isinstance(v, (int, float))}, indent=2))"`
   - Create the baseline JSON file with the output
   - Structure: `{"recall": X.XXXX, "precision": X.XXXX, "f2": X.XXXX, ...}`
   - Include freq_weighted_recall and risk_weighted_recall (calculated from entity_stats + weights)

2. Update `.github/workflows/test.yml`:
   - Add integration tests to the test job
   - Tiered approach:
     - On all pushes/PRs: Run existing tests PLUS `pytest tests/integration/test_regression.py -v` (quick smoke test)
     - On main branch: Also run `pytest tests/integration/test_full_evaluation.py -v` (full validation)
   - Add step for integration tests after existing pytest step
   - Ensure test.yml still passes all existing tests

The workflow change should look like:
```yaml
      - name: Run pytest
        run: pytest tests/ -v --tb=short

      - name: Run integration smoke tests
        run: pytest tests/integration/test_regression.py -v

      - name: Run full integration validation
        if: github.ref == 'refs/heads/main'
        run: pytest tests/integration/test_full_evaluation.py -v
```
  </action>
  <verify>
Run locally:
1. `pytest tests/integration/test_regression.py -v` - should pass with baseline
2. `pytest tests/integration/test_full_evaluation.py -v` - should pass
3. Verify baseline JSON exists and has correct structure
  </verify>
  <done>
Regression baseline committed. CI workflow updated with:
- Smoke tests (regression) on every push/PR
- Full validation on main branch merges
- Unweighted recall floor as hard CI failure condition
  </done>
</task>

</tasks>

<verification>
End-to-end validation:
1. `pytest tests/integration/ -v` passes all tests
2. `git diff .github/workflows/test.yml` shows tiered CI updates
3. `cat tests/baselines/regression.json` shows baseline metrics
4. Tests correctly validate CONF-01, CONF-02, CONF-03 requirements (config validation)
</verification>

<success_criteria>
Phase 16 Plan 01 complete when:
- [ ] Integration test directory exists with tests
- [ ] test_full_evaluation.py validates three metric types
- [ ] test_regression.py validates against baseline with 85% recall floor
- [ ] tests/baselines/regression.json committed with current metrics
- [ ] .github/workflows/test.yml updated with tiered integration tests
- [ ] All integration tests pass locally
</success_criteria>

<output>
After completion, create `.planning/phases/16-integration-validation/16-01-SUMMARY.md`
</output>
