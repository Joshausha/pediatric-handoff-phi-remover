---
phase: 16-integration-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/run_validation.py
  - tests/integration/__init__.py
  - tests/integration/test_full_evaluation.py
  - tests/integration/test_regression.py
  - tests/baselines/regression.json
  - .github/workflows/test.yml
autonomous: true

must_haves:
  truths:
    - "pytest tests/integration/ passes with all three metric types calculated"
    - "Unweighted recall >=85% threshold enforced as CI failure condition"
    - "CI workflow runs smoke tests on PRs and full validation on main branch"
    - "Regression baseline detects metric changes between runs"
  artifacts:
    - path: "tests/integration/test_full_evaluation.py"
      provides: "End-to-end validation orchestration"
      min_lines: 50
    - path: "tests/integration/test_regression.py"
      provides: "Regression baseline assertions"
      min_lines: 30
    - path: "tests/baselines/regression.json"
      provides: "Committed baseline metrics for regression detection"
      contains: "freq_weighted_recall"
    - path: ".github/workflows/test.yml"
      provides: "Tiered CI with smoke test and full validation"
      contains: "integration"
  key_links:
    - from: "tests/integration/test_full_evaluation.py"
      to: "tests/run_validation.py"
      via: "import run_validation"
      pattern: "from tests.run_validation import"
    - from: "tests/integration/test_full_evaluation.py"
      to: "run_validation return dict"
      via: "access results['metrics']['freq_weighted_recall']"
      pattern: "freq_weighted_recall"
    - from: "tests/integration/test_regression.py"
      to: "tests/baselines/regression.json"
      via: "json.load baseline file"
      pattern: "regression.json"
---

<objective>
Create integration test infrastructure for end-to-end validation of the dual-weight recall framework.

Purpose: Ensure the complete evaluation pipeline (run_validation.py -> weighted metrics -> report) works correctly and catches regressions before they reach production. This closes the validation loop for v2.2.

Output: Integration test suite with regression baselines, tiered CI workflow (smoke test on PRs, full validation on main).
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-integration-validation/16-CONTEXT.md

# Existing infrastructure to wrap
@tests/run_validation.py
@tests/evaluate_presidio.py
@tests/test_weighted_metrics.py
@app/config.py
@.github/workflows/test.yml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend run_validation.py to return weighted metrics</name>
  <files>
    tests/run_validation.py
  </files>
  <action>
Modify `run_validation()` to include entity_stats and pre-computed weighted metrics in the returned dict.

The `metrics` object returned by `evaluator.evaluate_dataset()` is an `EvaluationMetrics` instance with:
- `entity_stats`: Dict[str, Dict[str, int]] with per-entity {tp, fn, fp} counts
- `weighted_recall(weights)`, `weighted_precision(weights)`, `weighted_f2(weights)` methods
- `risk_weighted_recall(weights)`, etc.

**Add to the returned dict (after line 185, before the closing brace at 186):**

```python
# Import settings at top of function or use existing import
from app.config import settings

# After the existing metrics dict entries...

# Add entity stats for weighted metric calculation
"entity_stats": {
    entity_type: {
        "tp": stats["tp"],
        "fn": stats["fn"],
        "fp": stats["fp"],
    }
    for entity_type, stats in metrics.entity_stats.items()
},

# Add pre-computed weighted metrics
"freq_weighted_recall": float(metrics.weighted_recall(settings.spoken_handoff_weights)),
"freq_weighted_precision": float(metrics.weighted_precision(settings.spoken_handoff_weights)),
"freq_weighted_f2": float(metrics.weighted_f2(settings.spoken_handoff_weights)),
"risk_weighted_recall": float(metrics.risk_weighted_recall(settings.spoken_handoff_risk_weights)),
"risk_weighted_precision": float(metrics.risk_weighted_precision(settings.spoken_handoff_risk_weights)),
"risk_weighted_f2": float(metrics.risk_weighted_f2(settings.spoken_handoff_risk_weights)),
```

This ensures integration tests can access weighted metrics directly from `results["metrics"]["freq_weighted_recall"]` without needing to reconstruct the EvaluationMetrics object.
  </action>
  <verify>
Run: `python -c "from tests.run_validation import run_validation; from pathlib import Path; r = run_validation(Path('tests/synthetic_handoffs.json'), n_bootstrap=100, verbose=False); print('freq_weighted_recall:', r['metrics'].get('freq_weighted_recall', 'MISSING'))"`
Expected: Should print a float value like `freq_weighted_recall: 0.87...`
  </verify>
  <done>
run_validation() returns dict with entity_stats and all six pre-computed weighted metrics (freq_weighted_*, risk_weighted_*).
  </done>
</task>

<task type="auto">
  <name>Task 2: Create integration test infrastructure</name>
  <files>
    tests/integration/__init__.py
    tests/integration/test_full_evaluation.py
    tests/integration/test_regression.py
  </files>
  <action>
Create the integration test directory and files:

1. Create `tests/integration/__init__.py` (empty file for package)

2. Create `tests/integration/test_full_evaluation.py`:
   - Import `run_validation` from `tests.run_validation`
   - Use pytest fixture to run validation once and share across tests:
     ```python
     import pytest
     from pathlib import Path
     from tests.run_validation import run_validation
     from app.config import settings

     @pytest.fixture(scope="module")
     def validation_results():
         """Run validation once for all tests in module."""
         return run_validation(
             input_path=Path("tests/synthetic_handoffs.json"),
             n_bootstrap=1000,  # Reduced for speed
             verbose=False,
         )
     ```

   - Test `test_synthetic_dataset_evaluation(validation_results)`:
     - Assert `validation_results["metrics"]["recall"] >= 0.85` (unweighted recall floor - HARD FAIL)
     - Assert all three metric types present in metrics dict:
       - `recall` (unweighted)
       - `freq_weighted_recall`
       - `risk_weighted_recall`
     - Assert all metrics are float type

   - Test `test_three_metrics_calculated(validation_results)`:
     - Verify frequency-weighted and risk-weighted metrics differ
     - Document divergence observation in test docstring (not hard assertion per CONTEXT.md)

   - Test `test_config_weights_loaded()`:
     - Import settings from `app.config`
     - Assert `settings.spoken_handoff_weights` exists and has float values
     - Assert `settings.spoken_handoff_risk_weights` exists and has float values
     - Assert both dicts cover same entity types
     - (This VALIDATES that CONF-01, CONF-02, CONF-03 are implemented - they already exist in config.py)

3. Create `tests/integration/test_regression.py`:
   - Test `test_unweighted_recall_floor(validation_results)`:
     - Run validation and assert `recall >= 0.85` (this is the HIPAA safety floor)
     - Clear error message on failure: "REGRESSION: Unweighted recall {value:.1%} below 85% floor"

   - Test `test_metrics_match_baseline()`:
     - Load baseline from `tests/baselines/regression.json`
     - Run fresh validation (separate from fixture for baseline comparison)
     - Compare current metrics against baseline (with tolerance)
     - Use `pytest.approx()` for float comparisons with `rel=0.01` (1% tolerance)
     - Metrics to compare: recall, precision, f2, freq_weighted_recall, risk_weighted_recall

Use pytest fixtures for shared validation run (run once, use in multiple tests).
Do NOT use pytest-regressions library - use simple JSON file comparison instead (fewer dependencies).
  </action>
  <verify>
Run: `pytest tests/integration/ -v --tb=short`
Expected: Tests should run (may fail on missing baseline - Task 3 creates it)
  </verify>
  <done>
Integration tests exist and test structure is correct. Tests validate:
- Three metric types calculated correctly (from run_validation return dict)
- Unweighted recall floor enforced (>=85%)
- Config weights properly loaded from pydantic settings (validates CONF-01, CONF-02, CONF-03)
- Regression detection against committed baseline
  </done>
</task>

<task type="auto">
  <name>Task 3: Create regression baseline and CI workflow</name>
  <files>
    tests/baselines/regression.json
    .github/workflows/test.yml
  </files>
  <action>
1. Create `tests/baselines/` directory if it doesn't exist

2. Create `tests/baselines/regression.json`:
   - Run the validation once to get current metrics:
     ```bash
     python -c "
     from tests.run_validation import run_validation
     from pathlib import Path
     import json

     r = run_validation(Path('tests/synthetic_handoffs.json'), n_bootstrap=1000, verbose=False)
     baseline = {k: round(v, 4) for k, v in r['metrics'].items() if isinstance(v, (int, float))}
     print(json.dumps(baseline, indent=2, sort_keys=True))
     "
     ```
   - Create the baseline JSON file with the output
   - Structure should include: recall, precision, f2, freq_weighted_recall, freq_weighted_precision, freq_weighted_f2, risk_weighted_recall, risk_weighted_precision, risk_weighted_f2

3. Update `.github/workflows/test.yml`:
   - Add integration tests to the test job
   - Tiered approach:
     - On all pushes/PRs: Run existing tests PLUS `pytest tests/integration/test_regression.py -v` (quick smoke test)
     - On main branch: Also run `pytest tests/integration/test_full_evaluation.py -v` (full validation)
   - Add step for integration tests after existing pytest step
   - Ensure test.yml still passes all existing tests

The workflow change should look like:
```yaml
      - name: Run pytest
        run: pytest tests/ -v --tb=short

      - name: Run integration smoke tests
        run: pytest tests/integration/test_regression.py -v

      - name: Run full integration validation
        if: github.ref == 'refs/heads/main'
        run: pytest tests/integration/test_full_evaluation.py -v
```
  </action>
  <verify>
Run locally:
1. `pytest tests/integration/test_regression.py -v` - should pass with baseline
2. `pytest tests/integration/test_full_evaluation.py -v` - should pass
3. `cat tests/baselines/regression.json` - should show baseline metrics including weighted metrics
  </verify>
  <done>
Regression baseline committed with all weighted metrics. CI workflow updated with:
- Smoke tests (regression) on every push/PR
- Full validation on main branch merges
- Unweighted recall floor as hard CI failure condition
  </done>
</task>

</tasks>

<verification>
End-to-end validation:
1. `pytest tests/integration/ -v` passes all tests
2. `git diff .github/workflows/test.yml` shows tiered CI updates
3. `cat tests/baselines/regression.json` shows baseline metrics including freq_weighted_recall, risk_weighted_recall
4. Tests correctly validate CONF-01, CONF-02, CONF-03 requirements (config validation - they already exist)
5. run_validation.py returns entity_stats and weighted metrics in the dict
</verification>

<success_criteria>
Phase 16 Plan 01 complete when:
- [ ] run_validation.py updated to return entity_stats and weighted metrics
- [ ] Integration test directory exists with tests
- [ ] test_full_evaluation.py validates three metric types from returned dict
- [ ] test_regression.py validates against baseline with 85% recall floor
- [ ] tests/baselines/regression.json committed with current metrics (including weighted)
- [ ] .github/workflows/test.yml updated with tiered integration tests
- [ ] All integration tests pass locally
</success_criteria>

<output>
After completion, create `.planning/phases/16-integration-validation/16-01-SUMMARY.md`
</output>
