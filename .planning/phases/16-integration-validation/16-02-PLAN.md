---
phase: 16-integration-validation
plan: 02
type: execute
wave: 2
depends_on: ["16-01"]
files_modified:
  - tests/integration/generate_charts.py
  - tests/artifacts/.gitkeep
  - .planning/REQUIREMENTS.md
  - .planning/STATE.md
  - .planning/ROADMAP.md
autonomous: false

must_haves:
  truths:
    - "Metric comparison chart generates successfully as PNG"
    - "Chart shows unweighted vs frequency-weighted vs risk-weighted recall/precision"
    - "v2.2 milestone requirements all verified complete"
    - "ROADMAP.md shows Phase 16 complete with shipped v2.2"
  artifacts:
    - path: "tests/integration/generate_charts.py"
      provides: "Metric comparison visualization"
      min_lines: 40
    - path: "tests/artifacts/.gitkeep"
      provides: "Artifact output directory"
    - path: ".planning/REQUIREMENTS.md"
      provides: "All CONF requirements checked"
      contains: "[x] **CONF-01**"
  key_links:
    - from: "tests/integration/generate_charts.py"
      to: "tests/run_validation.py"
      via: "runs validation and extracts metrics"
      pattern: "run_validation"
---

<objective>
Generate metric comparison charts and finalize v2.2 milestone with human verification.

Purpose: Visual artifacts make divergence patterns visible for review. Human verification confirms the full dual-weight framework works correctly before marking v2.2 complete.

Output: Metric comparison chart (PNG), updated requirements/roadmap showing v2.2 shipped.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/REQUIREMENTS.md
@.planning/phases/16-integration-validation/16-01-SUMMARY.md

# Chart generation infrastructure
@tests/run_validation.py
@tests/evaluate_presidio.py
@app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create metric comparison chart generator</name>
  <files>
    tests/integration/generate_charts.py
    tests/artifacts/.gitkeep
  </files>
  <action>
1. Create `tests/artifacts/` directory with `.gitkeep` file
   - Add to `.gitignore`: `tests/artifacts/*.png` and `tests/artifacts/*.md` (gitignore generated files, keep directory)

2. Create `tests/integration/generate_charts.py`:
   - Function `generate_metric_comparison_chart(metrics: dict, output_path: Path) -> Path`
   - Use matplotlib (already in project deps)
   - Create grouped bar chart showing:
     - X-axis: Three metric types (Unweighted, Frequency-weighted, Risk-weighted)
     - Y-axis: Score (0-1.0)
     - Bars: Recall (green) and Precision (blue) side by side
     - Horizontal line at 0.85 (red dashed, labeled "85% threshold")
     - Value labels on top of each bar
   - Chart title: "Metric Comparison: Unweighted vs Weighted"
   - X-axis labels: "Unweighted\n(Safety Floor)", "Frequency-weighted\n(Spoken)", "Risk-weighted\n(Severity)"
   - Save as PNG with dpi=150

3. Add main block to generate chart:
   ```python
   if __name__ == "__main__":
       # Run validation and extract metrics
       from tests.run_validation import run_validation
       from pathlib import Path
       from app.config import settings

       results = run_validation(
           input_path=Path("tests/synthetic_handoffs.json"),
           n_bootstrap=1000,
           verbose=True,
       )

       # Need to get weighted metrics from entity_stats
       # (run_validation returns basic metrics, need to compute weighted)
       # ... compute freq_weighted_recall, risk_weighted_recall, etc.

       output_path = Path("tests/artifacts/metric_comparison.png")
       generate_metric_comparison_chart(metrics, output_path)
       print(f"Chart saved to {output_path}")
   ```

The chart should be runnable standalone: `python tests/integration/generate_charts.py`
  </action>
  <verify>
Run: `python tests/integration/generate_charts.py`
Expected: Chart generated at `tests/artifacts/metric_comparison.png`
Visual check: Chart shows three groups with recall/precision bars
  </verify>
  <done>
Chart generator exists and produces visual comparison of all three metric types.
Chart clearly shows unweighted as safety floor, frequency-weighted, and risk-weighted metrics.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update requirements and roadmap for v2.2 completion</name>
  <files>
    .planning/REQUIREMENTS.md
    .planning/STATE.md
    .planning/ROADMAP.md
  </files>
  <action>
1. Update `.planning/REQUIREMENTS.md`:
   - Mark CONF-01, CONF-02, CONF-03 as complete: `- [x] **CONF-XX**: ...`
   - These were validated by integration tests in Plan 01

2. Update `.planning/STATE.md`:
   - Update current position to Phase 16 complete
   - Update progress bar to 100% v2.2
   - Add v2.2 to Milestones Shipped table
   - Clear pending todos
   - Set next action to celebrate v2.2 ship

3. Update `.planning/ROADMAP.md`:
   - Mark Phase 16 plans complete: `- [x] 16-01-PLAN.md` and `- [x] 16-02-PLAN.md`
   - Update progress table: Phase 16 status = Complete
   - Move v2.2 section to completed (use details/summary pattern like v1.0, v2.0, v2.1)
  </action>
  <verify>
Check that all files updated correctly:
- `grep "CONF-01" .planning/REQUIREMENTS.md` shows `[x]`
- `grep "100%" .planning/STATE.md` shows v2.2 at 100%
- `grep "Phase 16" .planning/ROADMAP.md` shows Complete status
  </verify>
  <done>
All documentation updated to reflect v2.2 completion.
Requirements traceability shows all 20 v2.2 requirements complete.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete dual-weight recall framework with:
1. Integration test suite validating three metric types
2. Regression baseline with 85% recall floor enforcement
3. Tiered CI (smoke tests on PRs, full validation on main)
4. Metric comparison chart visualization
5. v2.2 milestone documentation complete
  </what-built>
  <how-to-verify>
1. Run full test suite: `pytest tests/ -v`
   - Expected: All tests pass (208+ passed)

2. Run integration tests: `pytest tests/integration/ -v`
   - Expected: All integration tests pass

3. Generate chart: `python tests/integration/generate_charts.py`
   - Expected: Chart at tests/artifacts/metric_comparison.png
   - Visual: Three groups showing unweighted > frequency > risk recall pattern

4. Review metrics in chart:
   - Unweighted recall should be ~86.4%
   - Frequency-weighted recall should be higher (more weight on high-recall entities)
   - Risk-weighted recall may be similar or lower (depends on entity distribution)

5. Verify CI would work: Review `.github/workflows/test.yml` changes

6. Check requirements: `grep "\[x\]" .planning/REQUIREMENTS.md | wc -l`
   - Expected: 20 checked requirements for v2.2
  </how-to-verify>
  <resume-signal>Type "approved" to mark v2.2 complete and commit, or describe any issues</resume-signal>
</task>

</tasks>

<verification>
Final v2.2 validation:
1. All 20 requirements checked complete in REQUIREMENTS.md
2. All tests pass (existing + integration)
3. Metric comparison chart generated successfully
4. ROADMAP.md shows v2.2 shipped
5. STATE.md reflects project completion
</verification>

<success_criteria>
Phase 16 Plan 02 complete when:
- [ ] Chart generator creates metric_comparison.png
- [ ] Chart shows three metric types with divergence visible
- [ ] REQUIREMENTS.md has all CONF-* checked complete
- [ ] STATE.md shows v2.2 at 100%
- [ ] ROADMAP.md shows Phase 16 complete
- [ ] Human verified full framework works correctly
</success_criteria>

<output>
After completion, create `.planning/phases/16-integration-validation/16-02-SUMMARY.md`
</output>
