---
phase: 16-integration-validation
plan: 02
type: execute
wave: 2
depends_on: ["16-01"]
files_modified:
  - tests/integration/generate_charts.py
  - tests/artifacts/.gitkeep
  - .planning/REQUIREMENTS.md
  - .planning/STATE.md
  - .planning/ROADMAP.md
autonomous: false

# Dependency note: This plan depends on 16-01 because:
# 1. run_validation.py must return weighted metrics (modified in 16-01 Task 1)
# 2. Integration tests must pass before marking v2.2 complete
# 3. The chart generator uses the same run_validation() output format

must_haves:
  truths:
    - "Metric comparison chart generates successfully as PNG"
    - "Chart shows unweighted vs frequency-weighted vs risk-weighted recall/precision"
    - "v2.2 milestone requirements all verified complete"
    - "ROADMAP.md shows Phase 16 complete with shipped v2.2"
  artifacts:
    - path: "tests/integration/generate_charts.py"
      provides: "Metric comparison visualization"
      min_lines: 40
    - path: "tests/artifacts/.gitkeep"
      provides: "Artifact output directory"
    - path: ".planning/REQUIREMENTS.md"
      provides: "All CONF requirements checked"
      contains: "[x] **CONF-01**"
  key_links:
    - from: "tests/integration/generate_charts.py"
      to: "tests/run_validation.py"
      via: "runs validation and extracts weighted metrics from returned dict"
      pattern: "freq_weighted_recall"
---

<objective>
Generate metric comparison charts and finalize v2.2 milestone with human verification.

Purpose: Visual artifacts make divergence patterns visible for review. Human verification confirms the full dual-weight framework works correctly before marking v2.2 complete.

Output: Metric comparison chart (PNG), updated requirements/roadmap showing v2.2 shipped.

**Precondition:** Plan 16-01 must have completed successfully (all integration tests passing). This plan depends on:
1. run_validation.py returning weighted metrics in the dict (16-01 Task 1)
2. Integration tests passing to validate the framework works (16-01 Tasks 2-3)
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/REQUIREMENTS.md
@.planning/phases/16-integration-validation/16-01-SUMMARY.md

# Chart generation infrastructure (after 16-01 modifications)
@tests/run_validation.py
@app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create metric comparison chart generator</name>
  <files>
    tests/integration/generate_charts.py
    tests/artifacts/.gitkeep
  </files>
  <action>
1. Create `tests/artifacts/` directory with `.gitkeep` file
   - Add to `.gitignore`: `tests/artifacts/*.png` and `tests/artifacts/*.md` (gitignore generated files, keep directory)

2. Create `tests/integration/generate_charts.py`:
   - Function `generate_metric_comparison_chart(metrics: dict, output_path: Path) -> Path`
   - Use matplotlib (already in project deps)
   - Create grouped bar chart showing:
     - X-axis: Three metric types (Unweighted, Frequency-weighted, Risk-weighted)
     - Y-axis: Score (0-1.0)
     - Bars: Recall (green) and Precision (blue) side by side
     - Horizontal line at 0.85 (red dashed, labeled "85% threshold")
     - Value labels on top of each bar
   - Chart title: "Metric Comparison: Unweighted vs Weighted"
   - X-axis labels: "Unweighted\n(Safety Floor)", "Frequency-weighted\n(Spoken)", "Risk-weighted\n(Severity)"
   - Save as PNG with dpi=150

3. Add main block to generate chart using the updated run_validation output:
   ```python
   if __name__ == "__main__":
       from tests.run_validation import run_validation
       from pathlib import Path

       # Run validation - now returns weighted metrics directly in the dict
       results = run_validation(
           input_path=Path("tests/synthetic_handoffs.json"),
           n_bootstrap=1000,
           verbose=True,
       )

       # Extract metrics directly from the returned dict
       # (16-01 Task 1 added these to the return value)
       metrics = {
           "recall": results["metrics"]["recall"],
           "precision": results["metrics"]["precision"],
           "freq_weighted_recall": results["metrics"]["freq_weighted_recall"],
           "freq_weighted_precision": results["metrics"]["freq_weighted_precision"],
           "risk_weighted_recall": results["metrics"]["risk_weighted_recall"],
           "risk_weighted_precision": results["metrics"]["risk_weighted_precision"],
       }

       output_path = Path("tests/artifacts/metric_comparison.png")
       generate_metric_comparison_chart(metrics, output_path)
       print(f"Chart saved to {output_path}")
   ```

The chart should be runnable standalone: `python tests/integration/generate_charts.py`

**Note:** This approach works because 16-01 Task 1 modified run_validation() to return freq_weighted_recall, risk_weighted_recall, etc. directly in the metrics dict. No need to reconstruct EvaluationMetrics or access entity_stats.
  </action>
  <verify>
Run: `python tests/integration/generate_charts.py`
Expected: Chart generated at `tests/artifacts/metric_comparison.png`
Visual check: Chart shows three groups with recall/precision bars
  </verify>
  <done>
Chart generator exists and produces visual comparison of all three metric types.
Chart clearly shows unweighted as safety floor, frequency-weighted, and risk-weighted metrics.
Uses weighted metrics directly from run_validation() return dict.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update requirements and roadmap for v2.2 completion</name>
  <files>
    .planning/REQUIREMENTS.md
    .planning/STATE.md
    .planning/ROADMAP.md
  </files>
  <action>
1. Update `.planning/REQUIREMENTS.md`:
   - Mark CONF-01, CONF-02, CONF-03 as complete: `- [x] **CONF-XX**: ...`
   - Note: These requirements were ALREADY IMPLEMENTED in app/config.py (lines 315-344)
   - Phase 16 Plan 01 test_config_weights_loaded() VALIDATED they exist
   - Now we mark them complete in the requirements tracking doc

2. Update `.planning/STATE.md`:
   - Update current position to Phase 16 complete
   - Update progress bar to 100% v2.2
   - Add v2.2 to Milestones Shipped table
   - Clear pending todos
   - Set next action to celebrate v2.2 ship

3. Update `.planning/ROADMAP.md`:
   - Mark Phase 16 plans complete: `- [x] 16-01-PLAN.md` and `- [x] 16-02-PLAN.md`
   - Update progress table: Phase 16 status = Complete
   - Move v2.2 section to completed (use details/summary pattern like v1.0, v2.0, v2.1)
  </action>
  <verify>
Check that all files updated correctly:
- `grep "CONF-01" .planning/REQUIREMENTS.md` shows `[x]`
- `grep "100%" .planning/STATE.md` shows v2.2 at 100%
- `grep "Phase 16" .planning/ROADMAP.md` shows Complete status
  </verify>
  <done>
All documentation updated to reflect v2.2 completion.
Requirements traceability shows all 20 v2.2 requirements complete.
CONF-01, CONF-02, CONF-03 marked complete (validated by integration tests in 16-01).
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete dual-weight recall framework with:
1. run_validation.py extended to return weighted metrics directly
2. Integration test suite validating three metric types
3. Regression baseline with 85% recall floor enforcement
4. Tiered CI (smoke tests on PRs, full validation on main)
5. Metric comparison chart visualization
6. v2.2 milestone documentation complete
  </what-built>
  <how-to-verify>
1. Run full test suite: `pytest tests/ -v`
   - Expected: All tests pass (208+ passed)

2. Run integration tests: `pytest tests/integration/ -v`
   - Expected: All integration tests pass

3. Generate chart: `python tests/integration/generate_charts.py`
   - Expected: Chart at tests/artifacts/metric_comparison.png
   - Visual: Three groups showing unweighted vs frequency-weighted vs risk-weighted

4. Review metrics in chart:
   - Unweighted recall should be ~86.4%
   - Frequency-weighted recall should be higher (more weight on high-recall entities)
   - Risk-weighted recall may be similar or lower (depends on entity distribution)

5. Verify weighted metrics in run_validation output:
   - `python -c "from tests.run_validation import run_validation; from pathlib import Path; r = run_validation(Path('tests/synthetic_handoffs.json'), n_bootstrap=100, verbose=False); print('freq_weighted_recall:', r['metrics']['freq_weighted_recall']); print('risk_weighted_recall:', r['metrics']['risk_weighted_recall'])"`
   - Expected: Both values should print as floats

6. Verify CI would work: Review `.github/workflows/test.yml` changes

7. Check requirements: `grep "\[x\]" .planning/REQUIREMENTS.md | wc -l`
   - Expected: 20 checked requirements for v2.2
  </how-to-verify>
  <resume-signal>Type "approved" to mark v2.2 complete and commit, or describe any issues</resume-signal>
</task>

</tasks>

<verification>
Final v2.2 validation:
1. All 20 requirements checked complete in REQUIREMENTS.md
2. All tests pass (existing + integration)
3. Metric comparison chart generated successfully
4. ROADMAP.md shows v2.2 shipped
5. STATE.md reflects project completion
6. run_validation() returns weighted metrics directly accessible in dict
</verification>

<success_criteria>
Phase 16 Plan 02 complete when:
- [ ] Chart generator creates metric_comparison.png using run_validation weighted metrics
- [ ] Chart shows three metric types with divergence visible
- [ ] REQUIREMENTS.md has all CONF-* checked complete (validated in 16-01, marked here)
- [ ] STATE.md shows v2.2 at 100%
- [ ] ROADMAP.md shows Phase 16 complete
- [ ] Human verified full framework works correctly
</success_criteria>

<output>
After completion, create `.planning/phases/16-integration-validation/16-02-SUMMARY.md`
</output>
